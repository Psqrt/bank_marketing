---
title: "bank-marketing"
author: "Psqrt"
date: "27/06/2019"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

# Introduction

This dataset is based on "Bank Marketing" UCI dataset (please check the description at: http://archive.ics.uci.edu/ml/datasets/Bank+Marketing). The data is enriched by the addition of five new social and economic features/attributes (national wide indicators from a ~10M population country), published by the Banco de Portugal and publicly available at: https://www.bportugal.pt/estatisticasweb. This dataset is almost identical to the one used in [Moro et al., 2014] (it does not include all attributes due to privacy concerns). 

The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The classification goal is to predict if the client will subscribe a term deposit (variable y).

## Dataset

### Available features

#### Bank client data:

* Age (numeric)
* Job : type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')
* Marital : marital status (categorical: 'divorced', 'married', 'single', 'unknown' ; note: 'divorced' means divorced or widowed)
* Education (categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')
* Default: has credit in default? (categorical: 'no', 'yes', 'unknown')
* Housing: has housing loan? (categorical: 'no', 'yes', 'unknown')
* Loan: has personal loan? (categorical: 'no', 'yes', 'unknown')

#### Related with the last contact of the current campaign:

* Contact: contact communication type (categorical: 'cellular','telephone')
* Month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
* Day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
* Duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

#### Other attributes:

* Campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
* Pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
* Previous: number of contacts performed before this campaign and for this client (numeric)
* Poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')

#### Social and economic context attributes:

* Emp.var.rate: employment variation rate - quarterly indicator (numeric)
* Cons.price.idx: consumer price index - monthly indicator (numeric)
* Cons.conf.idx: consumer confidence index - monthly indicator (numeric)
* Euribor3m: euribor 3 month rate - daily indicator (numeric)
* Nr.employed: number of employees - quarterly indicator (numeric)

#### Output variable (desired target):

* y: has the client subscribed a term deposit? (binary: 'yes', 'no')


### First insight

#### Loading packages ...

```{r, message=F}
library(tidyverse)
library(gmodels)
library(ggmosaic)
library(sjmisc)
library(questionr)
library(corrplot)
library(gridExtra)
library(ggpubr)
library(cowplot)
library(DMwR)
library(caret)
library(e1071)
library(ROCR)
library(plotROC)
library(pROC)
library(PRROC)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ranger)
library(lightgbm)
library(xgboost)
library(MLmetrics)
library(doMC)
registerDoMC(cores = 10)

select = dplyr::select
slice = dplyr::slice
```


#### Dataset importation

```{r}
bank_data = read.csv(file = "./data/bank-additional-full.csv",
                     sep = ";",
                     stringsAsFactors = F)
```

```{r}
dim(bank_data)
```

The dataset has 41,188 rows and 21 columns.

```{r}
names(bank_data)
```

The first 20 variables are our potential explanatory variables and the last one ("y") is the dependent variable.

```{r}
CrossTable(bank_data$y)
```

This is an unbalanced two-levels categorical variable, 88.7% of values taken are "no" (or "0") and only 11.3% of the remaining values are "yes" (or "1"). It is more natural to work with a 0/1 dependent variable:

```{r}
bank_data = bank_data %>% 
  mutate(y = factor(if_else(y == "yes", "1", "0"), 
                    levels = c("0", "1")))
```


```{r}
head(bank_data)
```

```{r}
sum(is.na(bank_data))
```

There's no missing value in the dataset. However, according to the data documentation, "unknown" value means NA.

```{r}
sum(bank_data == "unknown")
```

There are 12,718 unknown values in the dataset, let's try to find out which variables suffer the most from those NA values.

```{r}
bank_data %>% 
  summarise_all(list(~sum(. == "unknown"))) %>% 
  gather(key = "variable", value = "nr_unknown") %>% 
  arrange(-nr_unknown)
```

6 features have at least 1 unknown value. Before deciding how to manage those missing values, we'll study each variable and take a decision after visualisations. We can't afford to delete 8,597 rows in our dataset, it's more than 20% of our observations.


## Exploratory Analysis

### Univariate analysis 

Each feature will be checked one at a time. We'll eventually drop or transform some variables in order to clean up our dataset a little bit.

Those are a few functions that will be useful later.

```{r}
theme_set(theme_bw())

# setting default parameters for mosaic plots
mosaic_theme = theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank())

# setting default parameters for crosstables
fun_crosstable = function(df, var1, var2){
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

# plot weighted lm/leoss regressions with frequencies
fun_gg_freq = function(var){
  weight = table(bank_data[, var]) %>% 
    as.data.frame %>% 
    mutate(x = as.numeric(as.character(Var1))) %>% 
    select(-Var1) %>% 
    rename(weight = Freq)
  
  sink(tempfile())
  freq = fun_crosstable(bank_data, var, "y")$prop.r %>% 
    as.data.frame %>% 
    mutate(x = as.numeric(as.character(x)))
  sink()
  
  both = freq %>% 
    left_join(weight, by = "x") %>% 
    filter(weight > 50 & y == 1)
  
  gg = both %>% 
    ggplot() +
    aes(x = x,
        y = Freq,
        weight = weight) +
    geom_point(aes(size = weight)) +
    geom_smooth(aes(colour = "blue"), method = "loess") +
    geom_smooth(aes(colour = "red"), method = "lm", se = F) +
    coord_cartesian(ylim = c(-0.1, 1)) +
    theme(plot.margin = unit(c(0, 0, 0, 0), "pt")) +
    xlab(var) +
    ylab("") +
    scale_x_continuous(position = "top") +
    scale_colour_manual(values = c("blue", "red"),
                        labels = c("loess", "lm")) +
    labs(colour = "Regression")
  
  return(gg)
}

# re-ordering levels from factor variable
fun_reorder_levels = function(df, variable, first){
  remaining = unique(df[, variable])[which(unique(df[, variable]) != first)]
  x = factor(df[, variable], levels = c(first, remaining))
  return(x)
}

# plotting importance from predictive models into two panels
fun_imp_ggplot_split = function(model){
  if (class(model)[1] == "ranger"){
    imp_df = model$variable.importance %>% 
      data.frame("Overall" = .) %>% 
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  } else {
    imp_df = varImp(model) %>%
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  }
  
  gg1 = imp_df %>% 
    slice(1:floor(nrow(.)/2)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("Variables") +
    ylab("Importance") +
    theme(legend.position = "none")
  
  imp_range = ggplot_build(gg1)[["layout"]][["panel_params"]][[1]][["x.range"]]
  imp_gradient = scale_fill_gradient(limits = c(-imp_range[2], -imp_range[1]),
                                     low = "#132B43", 
                                     high = "#56B1F7")
  
  gg2 = imp_df %>% 
    slice(floor(nrow(.)/2)+1:nrow(.)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("") +
    ylab("Importance") +
    theme(legend.position = "none") +
    ylim(imp_range) +
    imp_gradient
  
  gg_both = plot_grid(gg1 + imp_gradient,
                      gg2)
  
  return(gg_both)
}

# plotting two performance measures
fun_gg_cutoff = function(score, obs, measure1, measure2) {
  predictions = prediction(score, obs)
  performance1 = performance(predictions, measure1)
  performance2 = performance(predictions, measure2)
  
  df1 = data.frame(x = performance1@x.values[[1]],
                   y = performance1@y.values[[1]],
                   measure = measure1,
                   stringsAsFactors = F) %>% 
    drop_na()
  df2 = data.frame(x = performance2@x.values[[1]],
                   y = performance2@y.values[[1]],
                   measure = measure2,
                   stringsAsFactors = F) %>% 
    drop_na()
  
  df = df1 %>% 
    bind_rows(df2)
  
  
  y_max_measure1 = max(df1$y, na.rm = T)
  x_max_measure1 = df1[df1$y == y_max_measure1, "x"][1]
  
  y_max_measure2 = max(df2$y, na.rm = T)
  x_max_measure2 = df2[df2$y == y_max_measure2, "x"][1]
  
  txt_measure1 = paste("Best cut for", measure1, ": x =", round(x_max_measure1, 3))
  txt_measure2 = paste("Best cut for", measure2, ": x =", round(x_max_measure2, 3))
  txt_tot = paste(txt_measure1, "\n", txt_measure2, sep = "")
  
  # browser()
  
  gg = df %>% 
    ggplot() +
    aes(x = x,
        y = y,
        colour = measure) +
    geom_line() +
    geom_vline(xintercept = c(x_max_measure1, x_max_measure2), linetype = "dashed", color = "gray") +
    geom_hline(yintercept = c(y_max_measure1, y_max_measure2), linetype = "dashed", color = "gray") +
    # annotate("text", x = 0.8, y = 0.2, hjust = 0, label = txt_measure1) +
    # annotate("text", x = 0.8, y = 0.1, hjust = 0, label = txt_measure2) +
    labs(caption = txt_tot) +
    theme(plot.caption = element_text(hjust = 0)) +
    xlim(c(0, 1)) +
    ylab("") +
    xlab("Threshold")
  
  return(gg)
}

# creating classes according to score and cut
fun_cut_predict = function(score, cut) {
  classes = score
  classes[classes > cut] = 1
  classes[classes <= cut] = 0
  classes = as.factor(classes)
  
  return(classes)  
}

aucpr = function(obs, score){
  
  df = data.frame("pred" = score,
                  "obs" = obs)
  
  prc = pr.curve(df[df$obs == 1, ]$pred,
                 df[df$obs == 0, ]$pred)
  
  return(prc$auc.davis.goadrich)
}

gg_prcurve = function(df) {
  # init
  df_gg = data.frame("v1" = numeric(), 
                     "v2" = numeric(), 
                     "v3" = numeric(), 
                     "model" = character(),
                     stringsAsFactors = F)
  
  # individual pr curves
  for (i in c(1:(ncol(df)-1))) {
    x1 = df[df$obs == 1, i]
    x2 = df[df$obs == 0, i]
    prc = pr.curve(x1, x2, curve = T)
    
    df_prc = as.data.frame(prc$curve, stringsAsFactors = F) %>% 
      mutate(model = colnames(df)[i])
    
    # combining pr curves
    df_gg = bind_rows(df_gg,
                      df_prc)
    
  }
  
  gg = df_gg %>% 
    ggplot() +
    aes(x = V1, y = V2, colour = model) +
    geom_line() +
    xlab("Recall") +
    ylab("Precision")
  
  return(gg)
}
```

#### Age

```{r}
summary(bank_data$age)
```

Ages range from 17 to 98, there doesn't seem anything strange from there. The other summary statistics are fine, the average is 40 years old.

```{r}
bank_data %>% 
  ggplot() +
  aes(x = age) +
  geom_bar() +
  geom_vline(xintercept = c(30, 60), 
             col = "red",
             linetype = "dashed") +
  facet_grid(y ~ .,
             scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 100, 5))
```

After the 60-years threshold, the relative frequency is higher when y = 1. In other words, we can say that elderly persons are more likely to subscribe to a term deposit.

```{r}
bank_data %>% 
  mutate(elder60 = if_else(age > 60, "1", "0")) %>% 
  group_by(y) %>% 
  add_count(nr_y = n()) %>% 
  group_by(elder60, y) %>% 
  summarise(abs_freq = n(),
            relative_freq = round(100*n()/first(nr_y), 2))
```

Elderly persons represent 8.92% of observations who accepted to subscribe to a term deposit, this proportion decreases to 1.36% for non subscribers.

We can also slice at 30 years to make three easily interpretable classes : [0, 30[, [30, 60[ and [60, +Inf[. The minimum and maximum values are 17 and 98 but we can expect new observations outside this range. We're replacing the continious variable "age" by this categorical variable.

```{r}
bank_data = bank_data %>% 
  mutate(age = if_else(age > 60, "high", if_else(age > 30, "mid", "low")))
```

Let's cross it with our dependent variable.

```{r}
fun_crosstable(bank_data, "age", "y")
```

45.5% of people over 60 years old subscribed a term deposit, which is a lot in comparison with younger individuals (15.2% for young adults (aged lower than 30) and only 9.4% for the remaining observations (aged between 30 and 60)).



#### Job

```{r}
table(bank_data$job)
```

There are mainly admin. (white-collars?) and blue-collars in the dataset. We can notice 330 "unknown" values.

```{r}
fun_crosstable(bank_data, "job", "y")
```

The "unknown" level doesn't show any important information and should be discarded from the data. We'll remove rows containing this value in the "job" column.

```{r}
bank_data = bank_data %>% 
  filter(job != "unknown")
```

```{r}
bank_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, job), fill = y)) +
  mosaic_theme +
  xlab("Job") +
  ylab(NULL)
```

Surprisingly, students (31.4%), retired people (25.2%) and unemployed (14.2%) categories show the best relative frequencies of term deposit subscription. Other levels range between 6.9% (blue-collar) and 13.0% (admin.).


#### Marital situation

```{r}
fun_crosstable(bank_data, "marital", "y")
```

For the same reasons as before, we'll remove rows with "unknown" as value for this variable.

```{r}
bank_data = bank_data %>% 
  filter(marital != "unknown")
```

Celibates slightly subscribe more often (14.0%) to term deposits than others (divorced (10.3%) and married (10.2%)).

```{r}
bank_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, marital), fill = y)) +
  mosaic_theme +
  xlab("Marital status") +
  ylab(NULL)
```



#### Education

```{r}
fun_crosstable(bank_data, "education", "y")
```

The illiterate category has not enough observations to be statisticaly meaningful. We can't discriminate illiterate people by using a pool made of 18 individuals only. Hence, those rows will be deleted from the dataset.

```{r}
bank_data = bank_data %>% 
  filter(education != "illiterate")
```


Among the 1,596 rows containing the "unknown" value, 234 of them subscribed to a term deposit. This is around 5% of the total group of subscribers. Since we're facing a very unbalanced dependent variable situation, we can not afford to discard those rows. Since this category has the highest relative frequency of "y = 1" (14.7%), we're going to add them in the "university.degree" level. This level has the second highest "y = 1" relative frequency (13.7%).

```{r}
bank_data = bank_data %>% 
  mutate(education = recode(education, "unknown" = "university.degree"))
```

```{r}
bank_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, education), fill = y)) +
  mosaic_theme +
  xlab("Education") +
  ylab(NULL)
```

It seems to be a positive correlation between the number of years of education and the odds to subscribe to a term deposit.



#### Default

```{r}
fun_crosstable(bank_data, "default", "y")
```

This feature is certainly not usable. Only 3 individuals replied "yes" to the question "Do you have credit in default?". People either answer "no" (79.3%) or don't even reply (20.7%), which gives zero information in our case. This variable is removed from the dataset.

```{r}
bank_data = bank_data %>% 
  select(-default)
```


#### Housing

```{r}
fun_crosstable(bank_data, "housing", "y")
```

```{r}
chisq.test(bank_data$housing, bank_data$y)
```

The p-value associated to the Chi-squared test equals to 0.065, which is higher than a 0.05-threshold. So, for a confidence level of 95%, there's no association between the dependent variable y and our feature housing. We're removing it from the dataset.

```{r}
bank_data = bank_data %>% 
  select(-housing)
```



#### Loan



```{r}
fun_crosstable(bank_data, "loan", "y")
```

```{r}
chisq.test(bank_data$loan, bank_data$y)
```

The p-value associated to the Chi-squared test equals to 0.648, which is higher than a 0.01-threshold. So, for a confidence level of 99%, there's no association between the dependent variable y and our feature loan. We're also removing it from the dataset.

```{r}
bank_data = bank_data %>% 
  select(-loan)
```


#### Contact

```{r}
fun_crosstable(bank_data, "contact", "y")
```

This feature is really interesting, 14.7% of cellular responders subscribed to a term deposit while only 5.2% of telephone responders did. I can't see any good reason to explain this.


#### Month

Recoding levels makes things easier.

```{r}
month_recode = c("jan" = "(01)jan",
                 "feb" = "(02)feb",
                 "mar" = "(03)mar",
                 "apr" = "(04)apr",
                 "may" = "(05)may",
                 "jun" = "(06)jun",
                 "jul" = "(07)jul",
                 "aug" = "(08)aug",
                 "sep" = "(09)sep",
                 "oct" = "(10)oct",
                 "nov" = "(11)nov",
                 "dec" = "(12)dec")

bank_data = bank_data %>% 
  mutate(month = recode(month, !!!month_recode))
```

```{r}
fun_crosstable(bank_data, "month", "y")
```

```{r}
bank_data %>% 
  ggplot() +
  aes(x = month, y = ..count../nrow(bank_data), fill = y) +
  geom_bar() +
  ylab("relative frequency")
```

First of all, we can notice that no contact has been made during January and February. The highest spike occurs during May, with 33.4% of total contacts, but it has the worst ratio of subscribers over persons contacted (6.5%). Every month with a very low frequency of contact (march, september, october and december) show very good results (between 44% and 51% of subscribers). Except maybe for december, there are enough observations to conclude this isn't pure luck, so this feature will probably be very important in models.


#### Day of the week

For the same reason as before, we're recoding the days of the week.

```{r}
day_recode = c("mon" = "(01)mon",
               "tue" = "(02)tue",
               "wed" = "(03)wed",
               "thu" = "(04)thu",
               "fri" = "(05)fri")

bank_data = bank_data %>% 
  mutate(day_of_week = recode(day_of_week, !!!day_recode))
```

```{r}
fun_crosstable(bank_data, "day_of_week", "y")
```

Calls aren't made during weekend days. If calls are evenly distributed between the different week days, Thursdays tend to show better results (12.1% of subscribers among calls made this day) unlike Mondays with only 10.0% of successful calls. However, those differences are small, which makes this feature not that important. It would've been interesting to see the attitude of responders from weekend calls.


#### Duration

Since the goal is to seek best candidates who will have the best odds to subscribe to a term deposit, the call duration can't be predicted before. So this feature is removed as recommended.

```{r}
bank_data = bank_data %>% 
  select(-duration)
```


#### Campaign

```{r}
bank_data %>% 
  ggplot() +
  aes(x = campaign) +
  geom_bar() +
  facet_grid(y ~ .,
             scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 50, 5))
```

Calling more than ten times a same person during a single marketing campaign seems excessive. We'll consider those as outliers, even if marketing harrassment a real thing. However, we can see that on the chart that harassment isn't working at all.

```{r}
bank_data = bank_data %>% 
  filter(campaign <= 10)
```


```{r}
bank_data %>% 
  ggplot() +
  aes(x = campaign) +
  geom_bar() +
  facet_grid(y ~ .,
             scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 10, 1))
```

```{r}
fun_crosstable(bank_data, "campaign", "y")
```

Let's extract the row proportions when y = 1.

```{r, echo = F}
prop_row = fun_crosstable(bank_data, "campaign", "y")$prop.row %>% 
  as.data.frame() %>% 
  filter(y == 1)
```

```{r}
prop_row %>% 
  ggplot() +
  aes(x = x,
      y = Freq) +
  geom_point() +
  geom_hline(yintercept = 0.085, 
             col = "red")
```

There's a linear pattern depending the different values of Campaign. We'll lose a lot of information if we're binning this variable. Hence, it'll kept as a categorical variable with 10 levels.

```{r}
bank_data = bank_data %>% 
  mutate(campaign = as.character(campaign))
```

All the variables are kept as character or numeric for now, we'll change them to factors if needed later.



#### Pdays

```{r}
table(bank_data$pdays)
```

This is the number of days that passed by after the client was last contacted from a previous campaign. 999 value means the client wasn't previously contacted. Let's make a dummy out of it.

```{r}
bank_data = bank_data %>% 
  mutate(pdays_dummy = if_else(pdays == 999, "0", "1")) %>% 
  select(-pdays)
```

```{r}
fun_crosstable(bank_data, "pdays_dummy", "y")
```

Recontacting a client after a previous campaign seems to highly increase the odds of subscription.


#### Previous

```{r}
table(bank_data$previous)
```

This is the number of contacts performed before this campaign and for each client.

```{r}
bank_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(previous), fill = y)) +
  mosaic_theme +
  xlab("Previous") +
  ylab(NULL)
```

If we make a dummy out of this, depending if the client has been contacted in a previous compaign (1, 2, ...) or not (0), this variable will exactly have the same information than pdays_dummy. We can't keep this variable without binning modalities because some levels show way not enough observations. The best we can do is to make 3 levels out of this.

```{r}
bank_data = bank_data %>% 
  mutate(previous = if_else(previous >=  2, "2+", if_else(previous == 1, "1", "0")))
```

This variable will still be highly correlated to pdays_dummy, because someone can be contacted more than once in a same compaign. It won't be surprising if we'll have to drop one of those variables.

```{r}
fun_crosstable(bank_data, "previous", "y")
```

As the analysis of the pdays_dummy variable has shown, recontacting someone again will increase the odds. Can we expect that long term harassment works unlike short terme harassment?



#### Poutcome

```{r}
fun_crosstable(bank_data, "poutcome", "y")
```

65.6% of people who already subscribed to a term deposit after a previous contact have accepted to do it again. Even if they refused before, they're still more enthusiastic to accept it (14.1%) than people who haven't been contacted before (9.0%). So even if the previous campaign was a failure, recontacting people is important.

<hr>

So far, the social and economic context attributes are remaining. Since those indicators are highly correlated together, we'll study those by pairs (bivariate study).

### Bivariate analysis

#### Employment variation rate, Consumer price index, Consumer confidence index, Euribor 3 months rate, Number of employees

Those five continious variables are social and economic indicators. They're supposed to be highly correlated. Let's compute the correlation matrix.

```{r}
bank_data %>% 
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed) %>% 
  cor() %>% 
  corrplot(method = "number",
           type = "upper",
           tl.cex = 0.8,
           tl.srt = 45,
           tl.col = "black")
```

As expected, three pairs show a correlation coefficient higher than 0.90 which is way too much. Our indicators are too correlated and share redundant information. Let's figure out which variable(s) can be removed to lighten this correlation matrix.


```{r, warning = F}
gg_emp.var.rate = fun_gg_freq("emp.var.rate")
gg_cons.price.idx = fun_gg_freq("cons.price.idx")
gg_cons.conf.idx = fun_gg_freq("cons.conf.idx")
gg_euribor3m = fun_gg_freq("euribor3m")
gg_nr.employed = fun_gg_freq("nr.employed")

plot_grid(gg_emp.var.rate + theme(legend.position = "none") + ylab("Frequency"), 
          gg_cons.price.idx + theme(legend.position = "none"),
          gg_cons.conf.idx + theme(legend.position = "none"),
          gg_euribor3m + theme(legend.position = "none"),
          gg_nr.employed + theme(legend.position = "none"),
          get_legend(gg_cons.conf.idx),
          align = "vh")

```

emp.var.rate isn't meaningful. We're removing it to soften correlations between those 5 variables.

```{r}
bank_data = bank_data %>% 
  select(-emp.var.rate)
```


```{r}
bank_data %>% 
  select(cons.price.idx, cons.conf.idx, euribor3m, nr.employed) %>% 
  cor() %>% 
  corrplot(method = "number",
           type = "full",
           tl.cex = 0.8,
           tl.srt = 45,
           tl.col = "black")
```

Even if there's still a high correlation between two variables : euribor3m and nr.employed (0.94), we're keeping both features. This is a spurious correlation, bank size (number of employees) isn't reactive to the euribor rate.



Let's continue the bivariate analysis by crossing our features with our predicted variable.

First, we'll split character features and non-character features (numeric or factor) to use appropriate tests (anova for numeric features and chi squared test (Cramer's V coefficient) for discrete variables).

```{r}
bank_data_x_dbl = bank_data %>%
  select_if(~{is.double(.) | is.factor(.)})

bank_data_x_chr = bank_data %>%
  select_if(~is.character(.))
```

Let's start with some anova.

```{r}
summary(aov(cons.price.idx ~ y,
            data = bank_data_x_dbl))
summary(aov(cons.conf.idx ~ y,
            data = bank_data_x_dbl))
summary(aov(euribor3m ~ y,
            data = bank_data_x_dbl))
summary(aov(nr.employed ~ y,
            data = bank_data_x_dbl))
```

All four remaining variables have their values statisticaly contrasted by the y response. This is good.

#### Cramer's V against y (3 columns)

The Cramer's V is the standardized value of the chi-squared test statistic and is calculated as follows:
$$V = \sqrt{\frac{\chi^2}{n \cdot (\min (r, c) -1)}}$$
whereby $n$ is the size of the sample, $r$ is the number of rows in our contingency table and $c$ is the number of columns. Since one of the two inputs of the contingency table is our predicted variable, which has only two levels, $\min (r,c) - 1$ is neccessarily equal to 1. So the formula is simply (in this case):

$$V = \sqrt{\frac{\chi^2}{n}}$$


```{r}
cramer = data.frame(NA, ncol(bank_data_x_chr), 3)

for (i in (1:ncol(bank_data_x_chr))){
  tab = table(bank_data_x_chr[, i], bank_data$y)
  chisq_results = chisq.test(tab)
  cramer[i, 1] = names(bank_data_x_chr)[i]
  # cramer[i, 2] = round(sqrt(chisq_results$statistic/(nrow(bank_data_x_chr) * (min(dim(tab)) -1))), 3)
  cramer[i, 2] = round(sqrt(chisq_results$statistic/(nrow(bank_data_x_chr))), 3)
  cramer[i, 3] = signif(chisq_results$p.value, 3)
}
colnames(cramer) = c("variable", "cramerv", "pvalue_chi2")
```



```{r}
cramer %>% 
  arrange(-cramerv) %>% 
  ggplot() +
  aes(x = reorder(variable, -cramerv),
      y = cramerv,
      fill = -cramerv) +
  geom_bar(stat = "identity",
           show.legend = F) +
  xlab("Variable") +
  ylab("Cramer's V") +
  ggtitle("Cramer's V against the explained response") +
  theme(axis.text.x = element_text(angle = 45,
                                   hjust = 1)) +
  scale_fill_gradient(high = "pink",
                      low = "darkred")
```

High V value means an important dependancy between the feature and the y variable.



#### Cramer's V matrix among features

This time, input can both have more than two levels, so the default formula is used:
$$V = \sqrt{\frac{\chi^2}{n \cdot (\min (r, c) -1)}}$$

```{r, warning=F, message=F}
cramer = matrix(NA, ncol(bank_data_x_chr), ncol(bank_data_x_chr))

for (i in (1:ncol(bank_data_x_chr))){
  for (j in (1:ncol(bank_data_x_chr))){
    tab = table(bank_data_x_chr[, i], bank_data_x_chr[, j])
    chisq_results = chisq.test(tab)
    cramer[i, j] = sqrt(chisq_results$statistic/(nrow(bank_data_x_chr) * (min(dim(tab)) -1)))
  }
}

cramer = round(cramer, 3)
colnames(cramer) = colnames(bank_data_x_chr)
rownames(cramer) = colnames(bank_data_x_chr)
```


```{r}
corrplot(cramer,
         method = "shade",
         type = "upper",
         diag = F,
         tl.srt = 45, 
         tl.col = "black",
         tl.cex = 0.6, 
         addCoef.col = "darkgreen", 
         addCoefasPercent = T)
```

Except of the pdays_dummy and poutcome variables, the standardized chi squared statistic isn't showing important dependecy between features. We're arbitrarily keeping both variables.

<hr>

So far, we've :

* removed four variables : default (lack of variability), housing (lack of information), loan (lack of information) and emp.var.rate (lack of significience),
* binned two variables : pdays and previous,
* framed one varaible : campaign because it had outliers,
* detected but kept two correlation issues : nr.employed with euribor3m and poutcome with pdays_dummy.

<hr>






# MODELS

## Data preparation

Now we've selected our features, it is useful to transform every character variable to factor variables for interpretation purposes for the logistic regressions. This function also picks the reference value for comparisons (reference group).

```{r}
bank_data$age = fun_reorder_levels(bank_data, "age", "low")
bank_data$job = fun_reorder_levels(bank_data, "job", "unemployed")
bank_data$marital = fun_reorder_levels(bank_data, "marital", "single")
bank_data$education = fun_reorder_levels(bank_data, "education", "basic.4y")
bank_data$contact = fun_reorder_levels(bank_data, "contact", "telephone")
bank_data$month = fun_reorder_levels(bank_data, "month", "(03)mar")
bank_data$day_of_week = fun_reorder_levels(bank_data, "day_of_week", "(01)mon")
bank_data$campaign = fun_reorder_levels(bank_data, "campaign", "1")
bank_data$previous = fun_reorder_levels(bank_data, "previous", "0")
bank_data$poutcome = fun_reorder_levels(bank_data, "poutcome", "nonexistent")
bank_data$pdays_dummy = fun_reorder_levels(bank_data, "pdays_dummy", "0")
```

Our dataset is ready, the only step remaining is splitting our data into training and validation sets. Since we're going to test different sampling methods, the training set will change between methods but the test set will remain the same to compare the different models. Sampling will be made with the `caret` package, except for the SMOTE, which will be made with the `DMwR` package.

```{r}
set.seed(1234)

ind = createDataPartition(bank_data$y,
                          times = 1,
                          p = 0.8,
                          list = F)
bank_train = bank_data[ind, ]
bank_test = bank_data[-ind, ]
```

## Normal sampling (unbalanced training set)

This is the default sampling, no change needs to be made.

```{r}
bank_train_ns = bank_train
bank_test_ns = bank_test
```

Let's start out with logistic regression, decision trees, random forests (through the `ranger` package), then boosting (Extreme GBM then Light GBM).



### Logistic regression

The first model is a logistic regression, with every remaining features.

```{r}
logistic_ns = glm(y ~ .,
                  data = bank_train_ns,
                  family = "binomial")
```

#### Results

```{r}
summary(logistic_ns)
```

A lot of features are non-significant in this model (including job, marital, education, previous, cons.price.idx and euribor3m). In a second time, we'll drop those features in order to build a more parsimonious model.

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_ns)
```

In line with what was said just before, a lot of variables don't show enough importance to the model. This first regression model can be tuned to be more accurate.

#### Predicted scores

Let's obtain the predicted scores for both datasets. The training scores are useful to evaluate how well did the training go and the validation scores (test scores) will be used to validate the model (cross-validation).

```{r, message=F, warning=F}
logistic_train_score_ns = predict(logistic_ns,
                                  newdata = bank_train_ns,
                                  type = "response")

logistic_test_score_ns = predict(logistic_ns,
                                 newdata = bank_test_ns,
                                 type = "response")
```

#### Cut identification

Let's see if the default threshold (0.5) is appropriate with our scores. But first, we need to figure out which metric we should use to evaluate the model's performance.

Since we've unbalanced data, it's easy to reach a huge accuracy score. Indeed, even the idiotic model which predicts the prevalent group for every observation will have a nice accuracy score. What's interesting is to predict correctly customers that are actually willing to subscribe to a term-deposit (True positive). Contacting a customer that isn't ready to subscribe (False positive) to it is bad, but it isn't as bad as skipping a potential customer (False negative). This will still cost time and money to the banks. So we want to focus on True positive rate, while having a reasonably low false positive rate.

Recall is the ratio of the correctly positive labeled people among people who are willing to subscribe in reality and Precision is the ratio of the correctly positive labeled people among all the positive labeled people. We want to have high scores for both rates. The F1 score (or F score) is fitting perfectly to our situation. It's the harmonic average of the precision and recall. This is what we're going to focus on, while keeping an eye to the accuracy rate.

```{r}
measure_train_ns = fun_gg_cutoff(logistic_train_score_ns, bank_train_ns$y, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.2, 0.5), 
             linetype = "dashed")
```

This plot shows the evolution of the accuracy and F1 rates according to the cut level. As it was said earlier, we want to have a good F1 score without dropping to much on accuracy. The 0.2 cut seems a good settlement, the trade-off between accuracy and F1 score is fair enough.

#### Confusion matrix - train data

Let's draw out the confusion matrix with the 0.2 cut.

```{r}
logistic_train_cut_ns = 0.2
logistic_train_class_ns = fun_cut_predict(logistic_train_score_ns, logistic_train_cut_ns)
# matrix
logistic_train_confm_ns = confusionMatrix(logistic_train_class_ns, bank_train_ns$y, 
                                          positive = "1",
                                          mode = "everything")
logistic_train_confm_ns
```

On the training set, the accuracy reachs 86.58% and the Sensitivity rate is close to 56.54%, which means that model manages to correctly label 86.58% of the times and 56.54% of the willingful customers are correctly detected. Let's evaluate this with the hold out data.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_ns = fun_gg_cutoff(logistic_test_score_ns, bank_test_ns$y, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(logistic_train_cut_ns, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_ns = fun_cut_predict(logistic_test_score_ns, logistic_train_cut_ns)
# matrix
logistic_test_confm_ns = confusionMatrix(logistic_test_class_ns, bank_test_ns$y, 
                                         positive = "1",
                                         mode = "everything")
logistic_test_confm_ns
```

The performance values are close to the training ones, our model doesn't suffer from over-fitting.


Let's apply the same procedure with other models.
















### Logistic regression 2 (simple)

We've find out that some features weren't relevant to our model, so let's drop those and remake a logistic regression.

```{r}
logistic_ns_2 = glm(y ~ . - job - marital - education - previous - euribor3m - cons.conf.idx - campaign,
                    data = bank_train_ns,
                    family = "binomial")
```

#### Results

```{r}
summary(logistic_ns_2)
```

This model seems as good as the previous one (according to the AIC), but with way lesser predictors.


#### Features importance

```{r}
fun_imp_ggplot_split(logistic_ns_2)
```

Every feature is now revelant to the model.

#### Predicted scores

Same as before, we compute scores to evaluate and validate our model.

```{r}
logistic_train_score_ns_2 = predict(logistic_ns_2,
                                    newdata = bank_train_ns,
                                    type = "response")

logistic_test_score_ns_2 = predict(logistic_ns_2,
                                   newdata = bank_test_ns,
                                   type = "response")
```

#### Cut identification

```{r}
measure_train_ns_2 = fun_gg_cutoff(logistic_train_score_ns_2, bank_train_ns$y, 
                                   "acc", "f")
measure_train_ns_2 +
  geom_vline(xintercept = c(0.2, 0.5), 
             linetype = "dashed")
```

The 0.2 threshold remains appropriate and is very close to the best cut for maximizing the F1 score.

#### Confusion matrix - train data

```{r}
logistic_train_cut_ns_2 = 0.2
logistic_train_class_ns_2 = fun_cut_predict(logistic_train_score_ns_2, logistic_train_cut_ns_2)
# matrix
logistic_train_confm_ns_2 = confusionMatrix(logistic_train_class_ns_2, bank_train_ns$y, 
                                            positive = "1",
                                            mode = "everything")
logistic_train_confm_ns_2
```

The accuracy slightly decreased but the sensitivity rate also slightly increased. Let's directly study the validation set.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_ns_2 = fun_gg_cutoff(logistic_test_score_ns_2, bank_test_ns$y, 
                                  "acc", "f")
measure_test_ns_2 +
  geom_vline(xintercept = c(logistic_train_cut_ns_2, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_ns_2 = fun_cut_predict(logistic_test_score_ns_2, logistic_train_cut_ns_2)
# matrix
logistic_test_confm_ns_2 = confusionMatrix(logistic_test_class_ns_2, bank_test_ns$y, 
                                           positive = "1",
                                           mode = "everything")
logistic_test_confm_ns_2
```

At the end, the second logistic regression performs almost as good as the first model, but since we're seeking of high sensitivity rate, this little increase is welcome.






### Decision Tree

```{r}
tree_ns = rpart(y ~ .,
                data = bank_train_ns,
                cp = 0.0005)
```


```{r}
rpart.plot(tree_ns)
```

Such a deep tree suffers from over-fitting, let's prune it according to its relative error evolution.

```{r}
plotcp(tree_ns)
```

```{r}
cp_best_ns = tree_ns$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_ns = prune(tree_ns,
                    cp = cp_best_ns)
```

```{r}
rpart.plot(tree_opt_ns)
```

This is the pruned tree which has the lowest relative error.

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_ns)
```

For the same reasons as before, we'll build another tree with lesser features in the next section.

#### Predicted scores

```{r}
tree_opt_train_score_ns = predict(tree_opt_ns,
                                  newdata = bank_train_ns,
                                  type = "prob")[, 2]

tree_opt_test_score_ns = predict(tree_opt_ns,
                                 newdata = bank_test_ns,
                                 type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_ns = fun_gg_cutoff(tree_opt_train_score_ns, bank_train_ns$y, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.25, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_ns = 0.25
tree_opt_train_class_ns = fun_cut_predict(tree_opt_train_score_ns, tree_opt_train_cut_ns)
tree_opt_train_confm_ns = confusionMatrix(tree_opt_train_class_ns, bank_train_ns$y, 
                                          positive = "1",
                                          mode = "everything")
tree_opt_train_confm_ns
```

The tree model is more accurate (around plus 2 percentage points) than the logistic ones on the training set, but its sensitivity has dropped (around minus 10 percentage points).

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(tree_opt_test_score_ns, bank_test_ns$y, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(tree_opt_train_cut_ns, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_ns = fun_cut_predict(tree_opt_test_score_ns, tree_opt_train_cut_ns)
# matrix
tree_opt_test_confm_ns = confusionMatrix(tree_opt_test_class_ns, bank_test_ns$y, 
                                         positive = "1",
                                         mode = "everything")
tree_opt_test_confm_ns
```

This decision tree suffers from over-fitting.






### Decision Tree 2

```{r}
tree_ns_2 = rpart(y ~ nr.employed + euribor3m + pdays_dummy + poutcome + month,
                  data = bank_train_ns,
                  cp = 0.0005)
```


```{r}
rpart.plot(tree_ns_2)
```

```{r}
plotcp(tree_ns_2)
```

```{r}
cp_best_ns_2 = tree_ns_2$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_ns_2 = prune(tree_ns_2,
                      cp = cp_best_ns_2)
```

```{r}
rpart.plot(tree_opt_ns_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_ns_2)
```

#### Predicted scores

```{r}
tree_opt_train_score_ns_2 = predict(tree_opt_ns_2,
                                    newdata = bank_train_ns,
                                    type = "prob")[, 2]

tree_opt_test_score_ns_2 = predict(tree_opt_ns_2,
                                   newdata = bank_test_ns,
                                   type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_ns_2 = fun_gg_cutoff(tree_opt_train_score_ns_2, bank_train_ns$y, 
                                   "acc", "f")
measure_train_ns_2 +
  geom_vline(xintercept = c(0.25, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_ns_2 = 0.25
tree_opt_train_class_ns_2 = fun_cut_predict(tree_opt_train_score_ns_2, tree_opt_train_cut_ns_2)
tree_opt_train_confm_ns_2 = confusionMatrix(tree_opt_train_class_ns_2, bank_train_ns$y, 
                                            positive = "1",
                                            mode = "everything")
tree_opt_train_confm_ns_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(tree_opt_test_score_ns_2, bank_test_ns$y, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(tree_opt_train_cut_ns_2, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_ns_2 = fun_cut_predict(tree_opt_test_score_ns_2, tree_opt_train_cut_ns_2)
# matrix
tree_opt_test_confm_ns_2 = confusionMatrix(tree_opt_test_class_ns_2, bank_test_ns$y, 
                                           positive = "1",
                                           mode = "everything")
tree_opt_test_confm_ns_2
```

This second decision tree is way better than the previous one, but the accuracy-sensitivity trade-off is delicate. We'll compare every models at the end.




### Random Forest (ranger)

```{r}
rf_ns = ranger(y ~ .,
               data = bank_train_ns,
               num.trees = 1000,
               importance = "impurity",
               write.forest = T,
               probability = T)
```


#### Results

```{r}
print(rf_ns)
```

#### Features importance

```{r}
fun_imp_ggplot_split(rf_ns)
```

#### Predicted scores

```{r}
rf_train_score_ns = predict(rf_ns,
                            data = bank_train_ns)$predictions[, 2]

rf_test_score_ns = predict(rf_ns,
                           data = bank_test_ns)$predictions[, 2]
```

#### Cut identification

```{r}
measure_train_ns = fun_gg_cutoff(rf_train_score_ns, bank_train_ns$y, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
rf_train_cut_ns = 0.3
rf_train_class_ns = fun_cut_predict(rf_train_score_ns, rf_train_cut_ns)
# matrix
rf_train_confm_ns = confusionMatrix(rf_train_class_ns, bank_train_ns$y, 
                                    positive = "1",
                                    mode = "everything")
rf_train_confm_ns
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(rf_test_score_ns, bank_test_ns$y, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(rf_train_cut_ns, 0.5), 
             linetype = "dashed")
```

```{r}
rf_test_class_ns = fun_cut_predict(rf_test_score_ns, rf_train_cut_ns)
# matrix
rf_test_confm_ns = confusionMatrix(rf_test_class_ns, bank_test_ns$y, 
                                   positive = "1",
                                   mode = "everything")
rf_test_confm_ns
```

### Light GBM

```{r}
bank_train_X_lgb_ns = as.matrix(lgb.prepare_rules(bank_train_ns %>% 
                                                    select(-y))[[1]])
bank_test_X_lgb_ns = as.matrix(lgb.prepare_rules(bank_test_ns %>% 
                                                   select(-y))[[1]])
bank_train_Y_lgb_ns = as.matrix(bank_train_ns %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_lgb_ns = as.matrix(bank_test_ns %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_lgb_ns = lgb.Dataset(bank_train_X_lgb_ns, 
                                label = bank_train_Y_lgb_ns)
bank_test_lgb_ns = lgb.Dataset(bank_test_X_lgb_ns, 
                               label = bank_test_Y_lgb_ns)

params_lgb = list(
  objective = "binary", # type of exercise
  metric = "auc", # metric to be evaluated 
  num_iterations = 500, # number of boosting iterations
  early_stopping_rounds = 200, # ill stop training if one metric of one validation data doesn't improve
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for tree model (used to deal with over-fitting when data is small)
  num_leaves = 7, # max number of leaves (nodes) in one tree
  is_unbalance = T, # unbalanced data?
  min_data_in_leaf = 10, # min number of data in one leaf (used to deal with over-fitting)
  feature_fraction = 0.9, # randomly select part of the features on each iteration
  bagging_fraction = 0.9, # randomly select part of the data without resampling
  bagging_freq = 1, # if != 0, enables bagging, performs bagging at every k iteration
  num_threads = 6 # number of cpu cores (not threads) to use
)

lgb_ns <- lgb.train(
  params = params_lgb,
  data = bank_train_lgb_ns,
  valids = list(train = bank_train_lgb_ns, 
                test = bank_test_lgb_ns),
  verbose = 1, # show results?
  eval_freq = 50 # show metric every how many iterations?
)
```

#### Predicted scores

```{r}
lgb_train_score_ns = predict(lgb_ns,
                             data = bank_train_X_lgb_ns,
                             n = lgb_ns$best_iter)

lgb_test_score_ns = predict(lgb_ns,
                            data = bank_test_X_lgb_ns,
                            n = lgb_ns$best_iter)
```

#### Cut identification

```{r}
measure_train_ns = fun_gg_cutoff(lgb_train_score_ns, bank_train_Y_lgb_ns, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
lgb_train_cut_ns = 0.75
lgb_train_class_ns = fun_cut_predict(lgb_train_score_ns, lgb_train_cut_ns)
lgb_train_confm_ns = confusionMatrix(lgb_train_class_ns, bank_train_ns$y, 
                                     positive = "1",
                                     mode = "everything")
lgb_train_confm_ns
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(lgb_test_score_ns, bank_test_Y_lgb_ns, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(lgb_train_cut_ns, 0.5), 
             linetype = "dashed")
```

```{r}
lgb_test_class_ns = fun_cut_predict(lgb_test_score_ns, lgb_train_cut_ns)
lgb_test_confm_ns = confusionMatrix(lgb_test_class_ns, bank_test_ns$y, 
                                    positive = "1",
                                    mode = "everything")
lgb_test_confm_ns
```


### XGBoost


```{r}
bank_train_X_xgb_ns = as.matrix(lgb.prepare_rules(bank_train_ns %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_ns = as.matrix(lgb.prepare_rules(bank_test_ns %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_ns = as.matrix(bank_train_ns %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_ns = as.matrix(bank_test_ns %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_ns = xgb.DMatrix(bank_train_X_xgb_ns, 
                                label = bank_train_Y_xgb_ns)
bank_test_xgb_ns = xgb.DMatrix(bank_test_X_xgb_ns, 
                               label = bank_test_Y_xgb_ns)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for trees
  subsample = 0.7, # subsample ratio per iteration
  colsample_bytree = 0.7, # sample of features per iteration
  min_child_weight = 5,
  tree_method = "hist", # tree construction algorithm
  max_bin = 10 # number of discrete bins to bucket continuous features
)

xgb_ns <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_ns,
  watchlist = list(train = bank_train_xgb_ns, 
                   test = bank_test_xgb_ns), # test as second data for xvalidation purposes
  nrounds = 1000,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = 200
)
```

#### Predicted scores

```{r}
xgb_train_score_ns = predict(xgb_ns,
                             newdata = bank_train_X_xgb_ns,
                             ntreelimit = xgb_ns$best_iteration)

xgb_test_score_ns = predict(xgb_ns,
                            newdata = bank_test_X_xgb_ns,
                            ntreelimit = xgb_ns$best_iteration)
```

#### Cut identification

```{r}
measure_train_ns = fun_gg_cutoff(xgb_train_score_ns, bank_train_Y_xgb_ns, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.25, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_ns = 0.25
xgb_train_class_ns = fun_cut_predict(xgb_train_score_ns, xgb_train_cut_ns)
xgb_train_confm_ns = confusionMatrix(xgb_train_class_ns, bank_train_ns$y, 
                                     positive = "1",
                                     mode = "everything")
xgb_train_confm_ns
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(xgb_test_score_ns, bank_test_Y_xgb_ns, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(xgb_train_cut_ns, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_ns = fun_cut_predict(xgb_test_score_ns, xgb_train_cut_ns)
xgb_test_confm_ns = confusionMatrix(xgb_test_class_ns, bank_test_ns$y, 
                                    positive = "1",
                                    mode = "everything")
xgb_test_confm_ns
```




### XGBoost hyperparameters search (caret)


```{r}
bank_X = as.matrix(lgb.prepare_rules(bank_data %>% 
                                       select(-y))[[1]])
bank_Y = bank_data$y

nrounds = 1000

tune_grid = expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = TRUE, # no training log
  allowParallel = FALSE # FALSE for reproducible results 
)

xgb_tune = train(
  x = bank_X,
  y = bank_Y,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune) +
  theme(legend.position = "bottom")
```



```{r}
tune_grid2 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
                     c(xgb_tune$bestTune$max_depth:4),
                     (xgb_tune$bestTune$max_depth - 1):(xgb_tune$bestTune$max_depth + 1)),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune2) +
  theme(legend.position = "bottom")
```

```{r}
tune_grid3 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune3) +
  theme(legend.position = "bottom")
```


```{r}
tune_grid4 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune4) +
  theme(legend.position = "bottom")
```


```{r}
tune_grid5 = expand.grid(
  nrounds = seq(from = 100, to = 2000, by = 100),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune5) +
  theme(legend.position = "bottom")
```


Now've every tuned hyperparameter.

```{r}
xgb_tune5$bestTune
```

Let's build our model.


```{r}
bank_train_X_xgb_ns = as.matrix(lgb.prepare_rules(bank_train_ns %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_ns = as.matrix(lgb.prepare_rules(bank_test_ns %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_ns = as.matrix(bank_train_ns %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_ns = as.matrix(bank_test_ns %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_ns = xgb.DMatrix(bank_train_X_xgb_ns, 
                                label = bank_train_Y_xgb_ns)
bank_test_xgb_ns = xgb.DMatrix(bank_test_X_xgb_ns, 
                               label = bank_test_Y_xgb_ns)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = xgb_tune5$bestTune$eta, # shrinkage rate
  max_depth = xgb_tune5$bestTune$max_depth, # max depth for trees
  subsample = xgb_tune5$bestTune$subsample, # subsample ratio per iteration
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree, # sample of features per iteration
  min_child_weight = xgb_tune5$bestTune$min_child_weight
)

xgb_ns_2 <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_ns,
  watchlist = list(train = bank_train_xgb_ns, 
                   test = bank_test_xgb_ns), # test as second data for xvalidation purposes
  nrounds = xgb_tune5$bestTune$nrounds,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = xgb_tune5$bestTune$nrounds
)
```

#### Predicted scores

```{r}
xgb_train_score_ns_2 = predict(xgb_ns_2,
                               newdata = bank_train_X_xgb_ns,
                               ntreelimit = xgb_ns_2$best_iteration)

xgb_test_score_ns_2 = predict(xgb_ns_2,
                              newdata = bank_test_X_xgb_ns,
                              ntreelimit = xgb_ns_2$best_iteration)
```

#### Cut identification

```{r}
measure_train_ns = fun_gg_cutoff(xgb_train_score_ns_2, bank_train_Y_xgb_ns, 
                                 "acc", "f")
measure_train_ns +
  geom_vline(xintercept = c(0.3, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_ns_2 = 0.3
xgb_train_class_ns_2 = fun_cut_predict(xgb_train_score_ns_2, xgb_train_cut_ns_2)
xgb_train_confm_ns_2 = confusionMatrix(xgb_train_class_ns_2, bank_train_ns$y, 
                                       positive = "1",
                                       mode = "everything")
xgb_train_confm_ns_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ns = fun_gg_cutoff(xgb_test_score_ns_2, bank_test_Y_xgb_ns, 
                                "acc", "f")
measure_test_ns +
  geom_vline(xintercept = c(xgb_train_cut_ns_2, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_ns_2 = fun_cut_predict(xgb_test_score_ns_2, xgb_train_cut_ns_2)
xgb_test_confm_ns_2 = confusionMatrix(xgb_test_class_ns_2, bank_test_ns$y, 
                                      positive = "1",
                                      mode = "everything")
xgb_test_confm_ns_2
```


### Model selection

#### ROC and PR curves - train


```{r}
score_train_ns = data.frame("logistic complex" = logistic_train_score_ns,
                            "logistic simple" = logistic_train_score_ns_2,
                            "tree complex" = tree_opt_train_score_ns,
                            "tree simple" = tree_opt_train_score_ns_2,
                            "random forest" = rf_train_score_ns,
                            "light gbm" = lgb_train_score_ns,
                            "xgboost default" = xgb_train_score_ns,
                            "xgboost tuned" = xgb_train_score_ns_2,
                            "obs" = as.numeric(bank_train_ns$y) - 1)


roc_train_ns = score_train_ns %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Train dataset")

prcurve_train_ns = gg_prcurve(score_train_ns) + ggtitle("PR Curve", subtitle = "Train dataset")

curves_train_ns = ggarrange(roc_train_ns, prcurve_train_ns, 
                            common.legend = T,
                            legend = "bottom")
```

```{r}
print(curves_train_ns)
```


#### Prediction quality - train

```{r, warning=F, message=F}
data.frame("Model" = c("Logistic regression (complex)",
                       "Logistic regression (simple)",
                       "Decision tree (complex)",
                       "Decision tree (simple)",
                       "Random forest (ranger)",
                       "Light GBM (default)",
                       "XGBoost (default)",
                       "XGBoost (tuned)"),
           "AUROC" = c(auc(bank_train_ns$y, logistic_train_score_ns),
                       auc(bank_train_ns$y, logistic_train_score_ns_2),
                       auc(bank_train_ns$y, tree_opt_train_score_ns),
                       auc(bank_train_ns$y, tree_opt_train_score_ns_2),
                       auc(bank_train_ns$y, rf_train_score_ns),
                       auc(bank_train_ns$y, lgb_train_score_ns),
                       auc(bank_train_ns$y, xgb_train_score_ns),
                       auc(bank_train_ns$y, xgb_train_score_ns_2)),
           "AUPR" = c(aucpr(bank_train_ns$y, logistic_train_score_ns),
                      aucpr(bank_train_ns$y, logistic_train_score_ns_2),
                      aucpr(bank_train_ns$y, tree_opt_train_score_ns),
                      aucpr(bank_train_ns$y, tree_opt_train_score_ns_2),
                      aucpr(bank_train_ns$y, rf_train_score_ns),
                      aucpr(bank_train_ns$y, lgb_train_score_ns),
                      aucpr(bank_train_ns$y, xgb_train_score_ns),
                      aucpr(bank_train_ns$y, xgb_train_score_ns_2)),
           "Cut" = c(logistic_train_cut_ns,
                     logistic_train_cut_ns_2,
                     tree_opt_train_cut_ns,
                     tree_opt_train_cut_ns_2,
                     rf_train_cut_ns,
                     lgb_train_cut_ns,
                     xgb_train_cut_ns,
                     xgb_train_cut_ns_2),
           "Accuracy" = c(logistic_train_confm_ns[["overall"]][["Accuracy"]],
                          logistic_train_confm_ns_2[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_ns[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_ns_2[["overall"]][["Accuracy"]],
                          rf_train_confm_ns[["overall"]][["Accuracy"]],
                          lgb_train_confm_ns[["overall"]][["Accuracy"]],
                          xgb_train_confm_ns[["overall"]][["Accuracy"]],
                          xgb_train_confm_ns_2[["overall"]][["Accuracy"]]),
           "F1" = c(logistic_train_confm_ns[["byClass"]][["F1"]],
                    logistic_train_confm_ns_2[["byClass"]][["F1"]],
                    tree_opt_train_confm_ns[["byClass"]][["F1"]],
                    tree_opt_train_confm_ns_2[["byClass"]][["F1"]],
                    rf_train_confm_ns[["byClass"]][["F1"]],
                    lgb_train_confm_ns[["byClass"]][["F1"]],
                    xgb_train_confm_ns[["byClass"]][["F1"]],
                    xgb_train_confm_ns_2[["byClass"]][["F1"]]),
           stringsAsFactors = F)
```

#### ROC CURVE - validation

```{r}
score_test_ns = data.frame("logistic complex" = logistic_test_score_ns,
                           "logistic simple" = logistic_test_score_ns_2,
                           "tree complex" = tree_opt_test_score_ns,
                           "tree simple" = tree_opt_test_score_ns_2,
                           "random forest" = rf_test_score_ns,
                           "light gbm" = lgb_test_score_ns,
                           "xgboost default" = xgb_test_score_ns,
                           "xgboost tuned" = xgb_test_score_ns_2,
                           "obs" = as.numeric(bank_test_ns$y) - 1)


roc_test_ns = score_test_ns %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Validation dataset")

prcurve_test_ns = gg_prcurve(score_test_ns) + ggtitle("PR Curve", subtitle = "Validation dataset")

curves_test_ns = ggarrange(roc_test_ns, prcurve_test_ns, 
                           common.legend = T,
                           legend = "bottom")
```

```{r}
print(curves_test_ns)
```




#### Prediction quality - validation

```{r, warning=F, message=F}
df_final_ns = data.frame("Model" = c("Logistic regression (complex)",
                                     "Logistic regression (simple)",
                                     "Decision tree (complex)",
                                     "Decision tree (simple)",
                                     "Random forest (ranger)",
                                     "Light GBM (default)",
                                     "XGBoost (default)",
                                     "XGBoost (tuned)"),
                         "AUROC" = c(auc(bank_test_ns$y, logistic_test_score_ns),
                                     auc(bank_test_ns$y, logistic_test_score_ns_2),
                                     auc(bank_test_ns$y, tree_opt_test_score_ns),
                                     auc(bank_test_ns$y, tree_opt_test_score_ns_2),
                                     auc(bank_test_ns$y, rf_test_score_ns),
                                     auc(bank_test_ns$y, lgb_test_score_ns),
                                     auc(bank_test_ns$y, xgb_test_score_ns),
                                     auc(bank_test_ns$y, xgb_test_score_ns_2)),
                         "AUPR" = c(aucpr(bank_test_ns$y, logistic_test_score_ns),
                                    aucpr(bank_test_ns$y, logistic_test_score_ns_2),
                                    aucpr(bank_test_ns$y, tree_opt_test_score_ns),
                                    aucpr(bank_test_ns$y, tree_opt_test_score_ns_2),
                                    aucpr(bank_test_ns$y, rf_test_score_ns),
                                    aucpr(bank_test_ns$y, lgb_test_score_ns),
                                    aucpr(bank_test_ns$y, xgb_test_score_ns),
                                    aucpr(bank_test_ns$y, xgb_test_score_ns_2)),
                         "Cut" = c(logistic_train_cut_ns,
                                   logistic_train_cut_ns_2,
                                   tree_opt_train_cut_ns,
                                   tree_opt_train_cut_ns_2,
                                   rf_train_cut_ns,
                                   lgb_train_cut_ns,
                                   xgb_train_cut_ns,
                                   xgb_train_cut_ns_2),
                         "Accuracy" = c(logistic_test_confm_ns[["overall"]][["Accuracy"]],
                                        logistic_test_confm_ns_2[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_ns[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_ns_2[["overall"]][["Accuracy"]],
                                        rf_test_confm_ns[["overall"]][["Accuracy"]],
                                        lgb_test_confm_ns[["overall"]][["Accuracy"]],
                                        xgb_test_confm_ns[["overall"]][["Accuracy"]],
                                        xgb_test_confm_ns_2[["overall"]][["Accuracy"]]),
                         "F1" = c(logistic_test_confm_ns[["byClass"]][["F1"]],
                                  logistic_test_confm_ns_2[["byClass"]][["F1"]],
                                  tree_opt_test_confm_ns[["byClass"]][["F1"]],
                                  tree_opt_test_confm_ns_2[["byClass"]][["F1"]],
                                  rf_test_confm_ns[["byClass"]][["F1"]],
                                  lgb_test_confm_ns[["byClass"]][["F1"]],
                                  xgb_test_confm_ns[["byClass"]][["F1"]],
                                  xgb_test_confm_ns_2[["byClass"]][["F1"]]),
                         stringsAsFactors = F)
df_final_ns %>% 
  arrange(-Accuracy, -F1)
```






























## !!! CASE 2 - DOWNSAMPLING SAMPLING 

```{r}
set.seed(1234)

bank_train_ds = downSample(x = bank_train %>% select(-y),
                           y = bank_train$y,
                           yname = "y")

bank_test_ds = bank_test
```









### Logistic regression

The first model is a logistic regression, with every remaining features.

```{r}
logistic_ds = glm(y ~ .,
                  data = bank_train_ds,
                  family = "binomial")
```

#### Results

```{r}
summary(logistic_ds)
```

A lot of features are non-significant in this model (including job, marital, education, previous, cons.price.idx and euribor3m). In a second time, we'll drop those features in order to build a more parsimonious model.

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_ds)
```

In line with what was said just before, a lot of variables don't show enough importance to the model. This first regression model can be tuned to be more accurate.

#### Predicted scores

Let's obtain the predicted scores for both datasets. The training scores are useful to evaluate how well did the training go and the validation scores (test scores) will be used to validate the model (cross-validation).

```{r, message=F, warning=F}
logistic_train_score_ds = predict(logistic_ds,
                                  newdata = bank_train_ds,
                                  type = "response")

logistic_test_score_ds = predict(logistic_ds,
                                 newdata = bank_test_ds,
                                 type = "response")
```

#### Cut identification

Let's see if the default threshold (0.5) is appropriate with our scores. But first, we need to figure out which metric we should use to evaluate the model's performance.

Since we've unbalanced data, it's easy to reach a huge accuracy score. Indeed, even the idiotic model which predicts the prevalent group for every observation will have a nice accuracy score. What's interesting is to predict correctly customers that are actually willing to subscribe to a term-deposit (True positive). Contacting a customer that isn't ready to subscribe (False positive) to it is bad, but it isn't as bad as skipping a potential customer (False negative). This will still cost time and money to the banks. So we want to focus on True positive rate, while having a reasonably low false positive rate.

Recall is the ratio of the correctly positive labeled people among people who are willing to subscribe in reality and Precision is the ratio of the correctly positive labeled people among all the positive labeled people. We want to have high scores for both rates. The F1 score (or F score) is fitting perfectly to our situation. It's the harmonic average of the precision and recall. This is what we're going to focus on, while keeping an eye to the accuracy rate.

```{r}
measure_train_ds = fun_gg_cutoff(logistic_train_score_ds, bank_train_ds$y, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

This plot shows the evolution of the accuracy and F1 rates according to the cut level. As it was said earlier, we want to have a good F1 score without dropping to much on accuracy. The 0.2 cut seems a good settlement, the trade-off between accuracy and F1 score is fair enough.

#### Confusion matrix - train data

Let's draw out the confusion matrix with the 0.2 cut.

```{r}
logistic_train_cut_ds = 0.75
logistic_train_class_ds = fun_cut_predict(logistic_train_score_ds, logistic_train_cut_ds)
# matrix
logistic_train_confm_ds = confusionMatrix(logistic_train_class_ds, bank_train_ds$y, 
                                          positive = "1",
                                          mode = "everything")
logistic_train_confm_ds
```

On the training set, the accuracy reachs 86.58% and the Sensitivity rate is close to 56.54%, which means that model manages to correctly label 86.58% of the times and 56.54% of the willingful customers are correctly detected. Let's evaluate this with the hold out data.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_ds = fun_gg_cutoff(logistic_test_score_ds, bank_test_ds$y, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(logistic_train_cut_ds, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_ds = fun_cut_predict(logistic_test_score_ds, logistic_train_cut_ds)
# matrix
logistic_test_confm_ds = confusionMatrix(logistic_test_class_ds, bank_test_ds$y, 
                                         positive = "1",
                                         mode = "everything")
logistic_test_confm_ds
```

The performance values are close to the training ones, our model doesn't suffer from over-fitting.


Let's apply the same procedure with other models.
















### Logistic regression 2 (simple)

We've find out that some features weren't relevant to our model, so let's drop those and remake a logistic regression.

```{r}
logistic_ds_2 = glm(y ~ . - marital - education - previous - campaign - cons.conf.idx - euribor3m,
                    data = bank_train_ds,
                    family = "binomial")
```

#### Results

```{r}
summary(logistic_ds_2)
```

This model seems as good as the previous one (according to the AIC), but with way lesser predictors.


#### Features importance

```{r}
fun_imp_ggplot_split(logistic_ds_2)
```

Every feature is now revelant to the model.

#### Predicted scores

Same as before, we compute scores to evaluate and validate our model.

```{r}
logistic_train_score_ds_2 = predict(logistic_ds_2,
                                    newdata = bank_train_ds,
                                    type = "response")

logistic_test_score_ds_2 = predict(logistic_ds_2,
                                   newdata = bank_test_ds,
                                   type = "response")
```

#### Cut identification

```{r}
measure_train_ds_2 = fun_gg_cutoff(logistic_train_score_ds_2, bank_train_ds$y, 
                                   "acc", "f")
measure_train_ds_2 +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

The 0.2 threshold remains appropriate and is very close to the best cut for maximizing the F1 score.

#### Confusion matrix - train data

```{r}
logistic_train_cut_ds_2 = 0.75
logistic_train_class_ds_2 = fun_cut_predict(logistic_train_score_ds_2, logistic_train_cut_ds_2)
# matrix
logistic_train_confm_ds_2 = confusionMatrix(logistic_train_class_ds_2, bank_train_ds$y, 
                                            positive = "1",
                                            mode = "everything")
logistic_train_confm_ds_2
```

The accuracy slightly decreased but the sensitivity rate also slightly increased. Let's directly study the validation set.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_ds_2 = fun_gg_cutoff(logistic_test_score_ds_2, bank_test_ds$y, 
                                  "acc", "f")
measure_test_ds_2 +
  geom_vline(xintercept = c(logistic_train_cut_ds_2, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_ds_2 = fun_cut_predict(logistic_test_score_ds_2, logistic_train_cut_ds_2)
# matrix
logistic_test_confm_ds_2 = confusionMatrix(logistic_test_class_ds_2, bank_test_ds$y, 
                                           positive = "1",
                                           mode = "everything")
logistic_test_confm_ds_2
```

At the end, the second logistic regression performs almost as good as the first model, but since we're seeking of high sensitivity rate, this little increase is welcome.






### Decision Tree

```{r}
tree_ds = rpart(y ~ .,
                data = bank_train_ds,
                cp = 0.0005)
```


```{r}
rpart.plot(tree_ds)
```

Such a deep tree suffers from over-fitting, let's prune it according to its relative error evolution.

```{r}
plotcp(tree_ds)
```

```{r}
cp_best_ds = tree_ds$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_ds = prune(tree_ds,
                    cp = cp_best_ds)
```

```{r}
rpart.plot(tree_opt_ds)
```

This is the pruned tree which has the lowest relative error.

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_ds)
```

For the same reasons as before, we'll build another tree with lesser features in the next section.

#### Predicted scores

```{r}
tree_opt_train_score_ds = predict(tree_opt_ds,
                                  newdata = bank_train_ds,
                                  type = "prob")[, 2]

tree_opt_test_score_ds = predict(tree_opt_ds,
                                 newdata = bank_test_ds,
                                 type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_ds = fun_gg_cutoff(tree_opt_train_score_ds, bank_train_ds$y, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_ds = 0.75
tree_opt_train_class_ds = fun_cut_predict(tree_opt_train_score_ds, tree_opt_train_cut_ds)
tree_opt_train_confm_ds = confusionMatrix(tree_opt_train_class_ds, bank_train_ds$y, 
                                          positive = "1",
                                          mode = "everything")
tree_opt_train_confm_ds
```

The tree model is more accurate (around plus 2 percentage points) than the logistic ones on the training set, but its sensitivity has dropped (around minus 10 percentage points).

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(tree_opt_test_score_ds, bank_test_ds$y, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(tree_opt_train_cut_ds, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_ds = fun_cut_predict(tree_opt_test_score_ds, tree_opt_train_cut_ds)
# matrix
tree_opt_test_confm_ds = confusionMatrix(tree_opt_test_class_ds, bank_test_ds$y, 
                                         positive = "1",
                                         mode = "everything")
tree_opt_test_confm_ds
```

This decision tree suffers from over-fitting.






### Decision Tree 2

```{r}
tree_ds_2 = rpart(y ~ nr.employed + euribor3m + pdays_dummy + poutcome + month + cons.conf.idx + cons.price.idx,
                  data = bank_train_ds,
                  cp = 0.0005)
```


```{r}
rpart.plot(tree_ds_2)
```

```{r}
plotcp(tree_ds_2)
```

```{r}
cp_best_ds_2 = tree_ds_2$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_ds_2 = prune(tree_ds_2,
                      cp = cp_best_ds_2)
```

```{r}
rpart.plot(tree_opt_ds_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_ds_2)
```

#### Predicted scores

```{r}
tree_opt_train_score_ds_2 = predict(tree_opt_ds_2,
                                    newdata = bank_train_ds,
                                    type = "prob")[, 2]

tree_opt_test_score_ds_2 = predict(tree_opt_ds_2,
                                   newdata = bank_test_ds,
                                   type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_ds_2 = fun_gg_cutoff(tree_opt_train_score_ds_2, bank_train_ds$y, 
                                   "acc", "f")
measure_train_ds_2 +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_ds_2 = 0.75
tree_opt_train_class_ds_2 = fun_cut_predict(tree_opt_train_score_ds_2, tree_opt_train_cut_ds_2)
tree_opt_train_confm_ds_2 = confusionMatrix(tree_opt_train_class_ds_2, bank_train_ds$y, 
                                            positive = "1",
                                            mode = "everything")
tree_opt_train_confm_ds_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(tree_opt_test_score_ds_2, bank_test_ds$y, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(tree_opt_train_cut_ds_2, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_ds_2 = fun_cut_predict(tree_opt_test_score_ds_2, tree_opt_train_cut_ds_2)
# matrix
tree_opt_test_confm_ds_2 = confusionMatrix(tree_opt_test_class_ds_2, bank_test_ds$y, 
                                           positive = "1",
                                           mode = "everything")
tree_opt_test_confm_ds_2
```

This second decision tree is way better than the previous one, but the accuracy-sensitivity trade-off is delicate. We'll compare every models at the end.




### Random Forest (ranger)

```{r}
rf_ds = ranger(y ~ .,
               data = bank_train_ds,
               num.trees = 1000,
               importance = "impurity",
               write.forest = T,
               probability = T)
```


#### Results

```{r}
print(rf_ds)
```

#### Features importance

```{r}
fun_imp_ggplot_split(rf_ds)
```

#### Predicted scores

```{r}
rf_train_score_ds = predict(rf_ds,
                            data = bank_train_ds)$predictions[, 2]

rf_test_score_ds = predict(rf_ds,
                           data = bank_test_ds)$predictions[, 2]
```

#### Cut identification

```{r}
measure_train_ds = fun_gg_cutoff(rf_train_score_ds, bank_train_ds$y, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
rf_train_cut_ds = 0.75
rf_train_class_ds = fun_cut_predict(rf_train_score_ds, rf_train_cut_ds)
# matrix
rf_train_confm_ds = confusionMatrix(rf_train_class_ds, bank_train_ds$y, 
                                    positive = "1",
                                    mode = "everything")
rf_train_confm_ds
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(rf_test_score_ds, bank_test_ds$y, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(rf_train_cut_ds, 0.5), 
             linetype = "dashed")
```

```{r}
rf_test_class_ds = fun_cut_predict(rf_test_score_ds, rf_train_cut_ds)
# matrix
rf_test_confm_ds = confusionMatrix(rf_test_class_ds, bank_test_ds$y, 
                                   positive = "1",
                                   mode = "everything")
rf_test_confm_ds
```

### Light GBM

```{r}
bank_train_X_lgb_ds = as.matrix(lgb.prepare_rules(bank_train_ds %>% 
                                                    select(-y))[[1]])
bank_test_X_lgb_ds = as.matrix(lgb.prepare_rules(bank_test_ds %>% 
                                                   select(-y))[[1]])
bank_train_Y_lgb_ds = as.matrix(bank_train_ds %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_lgb_ds = as.matrix(bank_test_ds %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_lgb_ds = lgb.Dataset(bank_train_X_lgb_ds, 
                                label = bank_train_Y_lgb_ds)
bank_test_lgb_ds = lgb.Dataset(bank_test_X_lgb_ds, 
                               label = bank_test_Y_lgb_ds)

params_lgb = list(
  objective = "binary", # type of exercise
  metric = "auc", # metric to be evaluated 
  num_iterations = 500, # number of boosting iterations
  early_stopping_rounds = 200, # ill stop training if one metric of one validation data doesn't improve
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for tree model (used to deal with over-fitting when data is small)
  num_leaves = 7, # max number of leaves (nodes) in one tree
  # scale_pos_weight = (1 - table(bank_train_ds$y)[[2]]/length(bank_train_ds$y)) * 100, # weight for positive class
  is_unbalance = T,
  min_data_in_leaf = 10, # min number of data in one leaf (used to deal with over-fitting)
  feature_fraction = 0.9, # randomly select part of the features on each iteration
  bagging_fraction = 0.9, # randomly select part of the data without resampling
  bagging_freq = 1, # if != 0, enables bagging, performs bagging at every k iteration
  num_threads = 6 # number of cpu cores (not threads) to use
)

lgb_ds <- lgb.train(
  params = params_lgb,
  data = bank_train_lgb_ds,
  valids = list(train = bank_train_lgb_ds, 
                test = bank_test_lgb_ds),
  verbose = 1, # show results?
  eval_freq = 50 # show metric every how many iterations?
)
```

#### Predicted scores

```{r}
lgb_train_score_ds = predict(lgb_ds,
                             data = bank_train_X_lgb_ds,
                             n = lgb_ds$best_iter)

lgb_test_score_ds = predict(lgb_ds,
                            data = bank_test_X_lgb_ds,
                            n = lgb_ds$best_iter)
```

#### Cut identification

```{r}
measure_train_ds = fun_gg_cutoff(lgb_train_score_ds, bank_train_Y_lgb_ds, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
lgb_train_cut_ds = 0.75
lgb_train_class_ds = fun_cut_predict(lgb_train_score_ds, lgb_train_cut_ds)
lgb_train_confm_ds = confusionMatrix(lgb_train_class_ds, bank_train_ds$y, 
                                     positive = "1",
                                     mode = "everything")
lgb_train_confm_ds
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(lgb_test_score_ds, bank_test_Y_lgb_ds, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(lgb_train_cut_ds, 0.5), 
             linetype = "dashed")
```

```{r}
lgb_test_class_ds = fun_cut_predict(lgb_test_score_ds, lgb_train_cut_ds)
lgb_test_confm_ds = confusionMatrix(lgb_test_class_ds, bank_test_ds$y, 
                                    positive = "1",
                                    mode = "everything")
lgb_test_confm_ds
```


### XGBoost


```{r}
bank_train_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_train_ds %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_test_ds %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_ds = as.matrix(bank_train_ds %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_ds = as.matrix(bank_test_ds %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_ds = xgb.DMatrix(bank_train_X_xgb_ds, 
                                label = bank_train_Y_xgb_ds)
bank_test_xgb_ds = xgb.DMatrix(bank_test_X_xgb_ds, 
                               label = bank_test_Y_xgb_ds)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for trees
  subsample = 0.7, # subsample ratio per iteration
  colsample_bytree = 0.7, # sample of features per iteration
  min_child_weight = 5,
  tree_method = "hist", # tree construction algorithm
  max_bin = 10 # number of discrete bins to bucket continuous features
)

xgb_ds <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_ds,
  watchlist = list(train = bank_train_xgb_ds, 
                   test = bank_test_xgb_ds), # test as second data for xvalidation purposes
  nrounds = 1000,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = 200
)
```

#### Predicted scores

```{r}
xgb_train_score_ds = predict(xgb_ds,
                             newdata = bank_train_X_xgb_ds,
                             ntreelimit = xgb_ds$best_iteration)

xgb_test_score_ds = predict(xgb_ds,
                            newdata = bank_test_X_xgb_ds,
                            ntreelimit = xgb_ds$best_iteration)
```

#### Cut identification

```{r}
measure_train_ds = fun_gg_cutoff(xgb_train_score_ds, bank_train_Y_xgb_ds, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_ds = 0.75
xgb_train_class_ds = fun_cut_predict(xgb_train_score_ds, xgb_train_cut_ds)
xgb_train_confm_ds = confusionMatrix(xgb_train_class_ds, bank_train_ds$y, 
                                     positive = "1",
                                     mode = "everything")
xgb_train_confm_ds
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(xgb_test_score_ds, bank_test_Y_xgb_ds, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(xgb_train_cut_ds, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_ds = fun_cut_predict(xgb_test_score_ds, xgb_train_cut_ds)
xgb_test_confm_ds = confusionMatrix(xgb_test_class_ds, bank_test_ds$y, 
                                    positive = "1",
                                    mode = "everything")
xgb_test_confm_ds
```




### XGBoost hyperparameters search (caret)


```{r}
bank_X = as.matrix(lgb.prepare_rules(bank_data %>% 
                                       select(-y))[[1]])
bank_Y = bank_data$y

nrounds = 1000

tune_grid = expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  verboseIter = TRUE, # no training log
  allowParallel = FALSE # FALSE for reproducible results 
)

xgb_tune = train(
  x = bank_X,
  y = bank_Y,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune) +
  theme(legend.position = "bottom")
```



```{r}
tune_grid2 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
                     c(xgb_tune$bestTune$max_depth:4),
                     (xgb_tune$bestTune$max_depth - 1):(xgb_tune$bestTune$max_depth + 1)),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune2) +
  theme(legend.position = "bottom")
```

```{r}
tune_grid3 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune3) +
  theme(legend.position = "bottom")
```


```{r, warning=F, message=F}
tune_grid4 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune4) +
  theme(legend.position = "bottom")
```


```{r}
tune_grid5 = expand.grid(
  nrounds = seq(from = 100, to = 2000, by = 100),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune5) +
  theme(legend.position = "bottom")
```


Now've every tuned hyperparameter.

```{r}
xgb_tune5$bestTune
```

Let's build our model.


```{r}
bank_train_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_train_ds %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_test_ds %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_ds = as.matrix(bank_train_ds %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_ds = as.matrix(bank_test_ds %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_ds = xgb.DMatrix(bank_train_X_xgb_ds, 
                                label = bank_train_Y_xgb_ds)
bank_test_xgb_ds = xgb.DMatrix(bank_test_X_xgb_ds, 
                               label = bank_test_Y_xgb_ds)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = xgb_tune5$bestTune$eta, # shrinkage rate
  max_depth = xgb_tune5$bestTune$max_depth, # max depth for trees
  subsample = xgb_tune5$bestTune$subsample, # subsample ratio per iteration
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree, # sample of features per iteration
  min_child_weight = xgb_tune5$bestTune$min_child_weight
)

xgb_ds_2 <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_ds,
  watchlist = list(train = bank_train_xgb_ds, 
                   test = bank_test_xgb_ds), # test as second data for xvalidation purposes
  nrounds = xgb_tune5$bestTune$nrounds,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = xgb_tune5$bestTune$nrounds
)
```

#### Predicted scores

```{r}
xgb_train_score_ds_2 = predict(xgb_ds_2,
                               newdata = bank_train_X_xgb_ds,
                               ntreelimit = xgb_ds_2$best_iteration)

xgb_test_score_ds_2 = predict(xgb_ds_2,
                              newdata = bank_test_X_xgb_ds,
                              ntreelimit = xgb_ds_2$best_iteration)
```

#### Cut identification

```{r}
measure_train_ds = fun_gg_cutoff(xgb_train_score_ds_2, bank_train_Y_xgb_ds, 
                                 "acc", "f")
measure_train_ds +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_ds_2 = 0.75
xgb_train_class_ds_2 = fun_cut_predict(xgb_train_score_ds_2, xgb_train_cut_ds_2)
xgb_train_confm_ds_2 = confusionMatrix(xgb_train_class_ds_2, bank_train_ds$y, 
                                       positive = "1",
                                       mode = "everything")
xgb_train_confm_ds_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_ds = fun_gg_cutoff(xgb_test_score_ds_2, bank_test_Y_xgb_ds, 
                                "acc", "f")
measure_test_ds +
  geom_vline(xintercept = c(xgb_train_cut_ds_2, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_ds_2 = fun_cut_predict(xgb_test_score_ds_2, xgb_train_cut_ds_2)
xgb_test_confm_ds_2 = confusionMatrix(xgb_test_class_ds_2, bank_test_ds$y, 
                                      positive = "1",
                                      mode = "everything")
xgb_test_confm_ds_2
```


### Model selection

#### ROC and PR curves - train


```{r}
score_train_ds = data.frame("logistic complex" = logistic_train_score_ds,
                            "logistic simple" = logistic_train_score_ds_2,
                            "tree complex" = tree_opt_train_score_ds,
                            "tree simple" = tree_opt_train_score_ds_2,
                            "random forest" = rf_train_score_ds,
                            "light gbm" = lgb_train_score_ds,
                            "xgboost default" = xgb_train_score_ds,
                            "xgboost tuned" = xgb_train_score_ds_2,
                            "obs" = as.numeric(bank_train_ds$y) - 1)


roc_train_ds = score_train_ds %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Train dataset")

prcurve_train_ds = gg_prcurve(score_train_ds) + ggtitle("PR Curve", subtitle = "Train dataset")

curves_train_ds = ggarrange(roc_train_ds, prcurve_train_ds, 
                            common.legend = T,
                            legend = "bottom")
```

```{r}
print(curves_train_ds)
```


#### Prediction quality - train

```{r, warning=F, message=F}
data.frame("Model" = c("Logistic regression (complex)",
                       "Logistic regression (simple)",
                       "Decision tree (complex)",
                       "Decision tree (simple)",
                       "Random forest (ranger)",
                       "Light GBM (default)",
                       "XGBoost (default)",
                       "XGBoost (tuned)"),
           "AUROC" = c(auc(bank_train_ds$y, logistic_train_score_ds),
                       auc(bank_train_ds$y, logistic_train_score_ds_2),
                       auc(bank_train_ds$y, tree_opt_train_score_ds),
                       auc(bank_train_ds$y, tree_opt_train_score_ds_2),
                       auc(bank_train_ds$y, rf_train_score_ds),
                       auc(bank_train_ds$y, lgb_train_score_ds),
                       auc(bank_train_ds$y, xgb_train_score_ds),
                       auc(bank_train_ds$y, xgb_train_score_ds_2)),
           "AUPR" = c(aucpr(bank_train_ds$y, logistic_train_score_ds),
                      aucpr(bank_train_ds$y, logistic_train_score_ds_2),
                      aucpr(bank_train_ds$y, tree_opt_train_score_ds),
                      aucpr(bank_train_ds$y, tree_opt_train_score_ds_2),
                      aucpr(bank_train_ds$y, rf_train_score_ds),
                      aucpr(bank_train_ds$y, lgb_train_score_ds),
                      aucpr(bank_train_ds$y, xgb_train_score_ds),
                      aucpr(bank_train_ds$y, xgb_train_score_ds_2)),
           "Cut" = c(logistic_train_cut_ds,
                     logistic_train_cut_ds_2,
                     tree_opt_train_cut_ds,
                     tree_opt_train_cut_ds_2,
                     rf_train_cut_ds,
                     lgb_train_cut_ds,
                     xgb_train_cut_ds,
                     xgb_train_cut_ds_2),
           "Accuracy" = c(logistic_train_confm_ds[["overall"]][["Accuracy"]],
                          logistic_train_confm_ds_2[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_ds[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_ds_2[["overall"]][["Accuracy"]],
                          rf_train_confm_ds[["overall"]][["Accuracy"]],
                          lgb_train_confm_ds[["overall"]][["Accuracy"]],
                          xgb_train_confm_ds[["overall"]][["Accuracy"]],
                          xgb_train_confm_ds_2[["overall"]][["Accuracy"]]),
           "F1" = c(logistic_train_confm_ds[["byClass"]][["F1"]],
                    logistic_train_confm_ds_2[["byClass"]][["F1"]],
                    tree_opt_train_confm_ds[["byClass"]][["F1"]],
                    tree_opt_train_confm_ds_2[["byClass"]][["F1"]],
                    rf_train_confm_ds[["byClass"]][["F1"]],
                    lgb_train_confm_ds[["byClass"]][["F1"]],
                    xgb_train_confm_ds[["byClass"]][["F1"]],
                    xgb_train_confm_ds_2[["byClass"]][["F1"]]),
           stringsAsFactors = F)
```

#### ROC CURVE - validation

```{r}
score_test_ds = data.frame("logistic complex" = logistic_test_score_ds,
                           "logistic simple" = logistic_test_score_ds_2,
                           "tree complex" = tree_opt_test_score_ds,
                           "tree simple" = tree_opt_test_score_ds_2,
                           "random forest" = rf_test_score_ds,
                           "light gbm" = lgb_test_score_ds,
                           "xgboost default" = xgb_test_score_ds,
                           "xgboost tuned" = xgb_test_score_ds_2,
                           "obs" = as.numeric(bank_test_ds$y) - 1)


roc_test_ds = score_test_ds %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Validation dataset")

prcurve_test_ds = gg_prcurve(score_test_ds) + ggtitle("PR Curve", subtitle = "Validation dataset")

curves_test_ds = ggarrange(roc_test_ds, prcurve_test_ds, 
                           common.legend = T,
                           legend = "bottom")
```

```{r}
print(curves_test_ds)
```




#### Prediction quality - validation

```{r, warning=F, message=F}
df_final_ds = data.frame("Model" = c("Logistic regression (complex)",
                                     "Logistic regression (simple)",
                                     "Decision tree (complex)",
                                     "Decision tree (simple)",
                                     "Random forest (ranger)",
                                     "Light GBM (default)",
                                     "XGBoost (default)",
                                     "XGBoost (tuned)"),
                         "AUROC" = c(auc(bank_test_ds$y, logistic_test_score_ds),
                                     auc(bank_test_ds$y, logistic_test_score_ds_2),
                                     auc(bank_test_ds$y, tree_opt_test_score_ds),
                                     auc(bank_test_ds$y, tree_opt_test_score_ds_2),
                                     auc(bank_test_ds$y, rf_test_score_ds),
                                     auc(bank_test_ds$y, lgb_test_score_ds),
                                     auc(bank_test_ds$y, xgb_test_score_ds),
                                     auc(bank_test_ds$y, xgb_test_score_ds_2)),
                         "AUPR" = c(aucpr(bank_test_ds$y, logistic_test_score_ds),
                                    aucpr(bank_test_ds$y, logistic_test_score_ds_2),
                                    aucpr(bank_test_ds$y, tree_opt_test_score_ds),
                                    aucpr(bank_test_ds$y, tree_opt_test_score_ds_2),
                                    aucpr(bank_test_ds$y, rf_test_score_ds),
                                    aucpr(bank_test_ds$y, lgb_test_score_ds),
                                    aucpr(bank_test_ds$y, xgb_test_score_ds),
                                    aucpr(bank_test_ds$y, xgb_test_score_ds_2)),
                         "Cut" = c(logistic_train_cut_ds,
                                   logistic_train_cut_ds_2,
                                   tree_opt_train_cut_ds,
                                   tree_opt_train_cut_ds_2,
                                   rf_train_cut_ds,
                                   lgb_train_cut_ds,
                                   xgb_train_cut_ds,
                                   xgb_train_cut_ds_2),
                         "Accuracy" = c(logistic_test_confm_ds[["overall"]][["Accuracy"]],
                                        logistic_test_confm_ds_2[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_ds[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_ds_2[["overall"]][["Accuracy"]],
                                        rf_test_confm_ds[["overall"]][["Accuracy"]],
                                        lgb_test_confm_ds[["overall"]][["Accuracy"]],
                                        xgb_test_confm_ds[["overall"]][["Accuracy"]],
                                        xgb_test_confm_ds_2[["overall"]][["Accuracy"]]),
                         "F1" = c(logistic_test_confm_ds[["byClass"]][["F1"]],
                                  logistic_test_confm_ds_2[["byClass"]][["F1"]],
                                  tree_opt_test_confm_ds[["byClass"]][["F1"]],
                                  tree_opt_test_confm_ds_2[["byClass"]][["F1"]],
                                  rf_test_confm_ds[["byClass"]][["F1"]],
                                  lgb_test_confm_ds[["byClass"]][["F1"]],
                                  xgb_test_confm_ds[["byClass"]][["F1"]],
                                  xgb_test_confm_ds_2[["byClass"]][["F1"]]),
                         stringsAsFactors = F)
df_final_ds %>% 
  arrange(-Accuracy, -F1)
```





























































<!-- ### Logistic regression -->

<!-- ```{r} -->
<!-- logistic_ds = glm(y ~ ., -->
<!--                   data = bank_train_ds, -->
<!--                   family = "binomial") -->
<!-- ``` -->

<!-- #### Results -->

<!-- ```{r} -->
<!-- summary(logistic_ds) -->
<!-- ``` -->

<!-- #### Features importance -->

<!-- ```{r} -->
<!-- fun_imp_ggplot_split(logistic_ds) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- logistic_train_score_ds = predict(logistic_ds, -->
<!--                                   newdata = bank_train_ds, -->
<!--                                   type = "response") -->

<!-- logistic_test_score_ds = predict(logistic_ds, -->
<!--                                  newdata = bank_test_ds, -->
<!--                                  type = "response") -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds = fun_gg_cutoff(logistic_train_score_ds, bank_train_ds$y,  -->
<!--                                  "acc", "sens") -->
<!-- measure_train_ds + -->
<!--   geom_vline(xintercept = c(0.375, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- #### Confusion matrix - train data -->

<!-- ```{r} -->
<!-- logistic_train_class_ds = fun_cut_predict(logistic_train_score_ds, 0.5) -->
<!-- # matrix -->
<!-- logistic_train_confm_ds = confusionMatrix(logistic_train_class_ds, bank_train_ds$y,  -->
<!--                                           positive = "1") -->
<!-- logistic_train_confm_ds -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->

<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(logistic_test_score_ds, bank_test_ds$y,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.375, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- logistic_test_class_ds = fun_cut_predict(logistic_test_score_ds, 0.5) -->
<!-- # matrix -->
<!-- logistic_test_confm_ds = confusionMatrix(logistic_test_class_ds, bank_test_ds$y,  -->
<!--                                          positive = "1") -->
<!-- logistic_test_confm_ds -->
<!-- ``` -->








<!-- ### Logistic regression 2 (simple) -->

<!-- ```{r} -->
<!-- logistic_ds_2 = glm(y ~ . - marital - education - previous - campaign - cons.conf.idx - euribor3m, -->
<!--                     data = bank_train_ds, -->
<!--                     family = "binomial") -->
<!-- ``` -->

<!-- #### Results -->

<!-- ```{r} -->
<!-- summary(logistic_ds_2) -->
<!-- ``` -->

<!-- #### Features importance -->

<!-- ```{r} -->
<!-- fun_imp_ggplot_split(logistic_ds_2) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- logistic_train_score_ds_2 = predict(logistic_ds_2, -->
<!--                                     newdata = bank_train_ds, -->
<!--                                     type = "response") -->

<!-- logistic_test_score_ds_2 = predict(logistic_ds_2, -->
<!--                                    newdata = bank_test_ds, -->
<!--                                    type = "response") -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds_2 = fun_gg_cutoff(logistic_train_score_ds_2, bank_train_ds$y,  -->
<!--                                    "acc", "sens") -->
<!-- measure_train_ds_2 + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- #### Confusion matrix - train data -->

<!-- ```{r} -->
<!-- logistic_train_class_ds_2 = fun_cut_predict(logistic_train_score_ds_2, 0.5) -->
<!-- # matrix -->
<!-- logistic_train_confm_ds_2 = confusionMatrix(logistic_train_class_ds_2, bank_train_ds$y,  -->
<!--                                             positive = "1") -->
<!-- logistic_train_confm_ds_2 -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->

<!-- ```{r} -->
<!-- measure_test_ds_2 = fun_gg_cutoff(logistic_test_score_ds_2, bank_test_ds$y,  -->
<!--                                   "acc", "sens") -->
<!-- measure_test_ds_2 + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- logistic_test_class_ds_2 = fun_cut_predict(logistic_test_score_ds_2, 0.5) -->
<!-- # matrix -->
<!-- logistic_test_confm_ds_2 = confusionMatrix(logistic_test_class_ds_2, bank_test_ds$y,  -->
<!--                                            positive = "1") -->
<!-- logistic_test_confm_ds_2 -->
<!-- ``` -->






<!-- ### Decision Tree -->

<!-- ```{r} -->
<!-- tree_ds = rpart(y ~ ., -->
<!--                 data = bank_train_ds, -->
<!--                 cp = 0.0005) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- rpart.plot(tree_ds) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- plotcp(tree_ds) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- cp_best_ds = tree_ds$cptable %>% -->
<!--   as.data.frame %>% -->
<!--   filter(xerror == min(xerror)) %>% -->
<!--   select(CP) %>% -->
<!--   slice(1) %>% -->
<!--   as.numeric() -->

<!-- tree_opt_ds = prune(tree_ds, -->
<!--                     cp = cp_best_ds) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- rpart.plot(tree_opt_ds) -->
<!-- ``` -->

<!-- #### Features importance -->

<!-- ```{r} -->
<!-- fun_imp_ggplot_split(tree_opt_ds) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- tree_opt_train_score_ds = predict(tree_opt_ds, -->
<!--                                   newdata = bank_train_ds, -->
<!--                                   type = "prob")[, 2] -->

<!-- tree_opt_test_score_ds = predict(tree_opt_ds, -->
<!--                                  newdata = bank_test_ds, -->
<!--                                  type = "prob")[, 2] -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds = fun_gg_cutoff(tree_opt_train_score_ds, bank_train_ds$y,  -->
<!--                                  "acc", "sens") -->
<!-- measure_train_ds + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- #### Confusion matrix - train data -->


<!-- ```{r} -->
<!-- tree_opt_train_class_ds = fun_cut_predict(tree_opt_train_score_ds, 0.5) -->
<!-- tree_opt_train_confm_ds = confusionMatrix(tree_opt_train_class_ds, bank_train_ds$y,  -->
<!--                                           positive = "1") -->
<!-- tree_opt_train_confm_ds -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->


<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(tree_opt_test_score_ds, bank_test_ds$y,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tree_opt_test_class_ds = fun_cut_predict(tree_opt_test_score_ds, 0.5) -->
<!-- # matrix -->
<!-- tree_opt_test_confm_ds = confusionMatrix(tree_opt_test_class_ds, bank_test_ds$y,  -->
<!--                                          positive = "1") -->
<!-- tree_opt_test_confm_ds -->
<!-- ``` -->








<!-- ### Decision Tree 2 -->

<!-- ```{r} -->
<!-- tree_ds_2 = rpart(y ~ nr.employed + euribor3m + cons.conf.idx + pdays_dummy + poutcome + cons.price.idx + month, -->
<!--                   data = bank_train_ds, -->
<!--                   cp = 0.0005) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- rpart.plot(tree_ds_2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- plotcp(tree_ds_2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- cp_best_ds_2 = tree_ds_2$cptable %>% -->
<!--   as.data.frame %>% -->
<!--   filter(xerror == min(xerror)) %>% -->
<!--   select(CP) %>% -->
<!--   slice(1) %>% -->
<!--   as.numeric() -->

<!-- tree_opt_ds_2 = prune(tree_ds_2, -->
<!--                       cp = cp_best_ds_2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- rpart.plot(tree_opt_ds_2) -->
<!-- ``` -->

<!-- #### Features importance -->

<!-- ```{r} -->
<!-- fun_imp_ggplot_split(tree_opt_ds_2) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- tree_opt_train_score_ds_2 = predict(tree_opt_ds_2, -->
<!--                                     newdata = bank_train_ds, -->
<!--                                     type = "prob")[, 2] -->

<!-- tree_opt_test_score_ds_2 = predict(tree_opt_ds_2, -->
<!--                                    newdata = bank_test_ds, -->
<!--                                    type = "prob")[, 2] -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds_2 = fun_gg_cutoff(tree_opt_train_score_ds_2, bank_train_ds$y,  -->
<!--                                    "acc", "sens") -->
<!-- measure_train_ds_2 + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- #### Confusion matrix - train data -->


<!-- ```{r} -->
<!-- tree_opt_train_class_ds_2 = fun_cut_predict(tree_opt_train_score_ds_2, 0.5) -->
<!-- tree_opt_train_confm_ds_2 = confusionMatrix(tree_opt_train_class_ds_2, bank_train_ds$y,  -->
<!--                                             positive = "1") -->
<!-- tree_opt_train_confm_ds_2 -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->


<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(tree_opt_test_score_ds_2, bank_test_ds$y,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tree_opt_test_class_ds_2 = fun_cut_predict(tree_opt_test_score_ds_2, 0.5) -->
<!-- # matrix -->
<!-- tree_opt_test_confm_ds_2 = confusionMatrix(tree_opt_test_class_ds_2, bank_test_ds$y,  -->
<!--                                            positive = "1") -->
<!-- tree_opt_test_confm_ds_2 -->
<!-- ``` -->






<!-- ### Random Forest (ranger) -->

<!-- ```{r} -->
<!-- rf_ds = ranger(y ~ ., -->
<!--                data = bank_train_ds, -->
<!--                num.trees = 1000, -->
<!--                importance = "impurity", -->
<!--                write.forest = T, -->
<!--                probability = T) -->
<!-- ``` -->


<!-- #### Results -->

<!-- ```{r} -->
<!-- print(rf_ds) -->
<!-- ``` -->

<!-- #### Features importance -->

<!-- ```{r} -->
<!-- fun_imp_ggplot_split(rf_ds) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- rf_train_score_ds = predict(rf_ds, -->
<!--                             data = bank_train_ds)$predictions[, 2] -->

<!-- rf_test_score_ds = predict(rf_ds, -->
<!--                            data = bank_test_ds)$predictions[, 2] -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds = fun_gg_cutoff(rf_train_score_ds, bank_train_ds$y,  -->
<!--                                  "acc", "sens") -->
<!-- measure_train_ds + -->
<!--   geom_vline(xintercept = c(0.375, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- #### Confusion matrix - train data -->

<!-- ```{r} -->
<!-- rf_train_class_ds = fun_cut_predict(rf_train_score_ds, 0.375) -->
<!-- # matrix -->
<!-- rf_train_confm_ds = confusionMatrix(rf_train_class_ds, bank_train_ds$y,  -->
<!--                                     positive = "1") -->
<!-- rf_train_confm_ds -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->


<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(rf_test_score_ds, bank_test_ds$y,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.375, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- rf_test_class_ds = fun_cut_predict(rf_test_score_ds, 0.5) -->
<!-- # matrix -->
<!-- rf_test_confm_ds = confusionMatrix(rf_test_class_ds, bank_test_ds$y,  -->
<!--                                    positive = "1") -->
<!-- rf_test_confm_ds -->
<!-- ``` -->







<!-- ### Light GBM -->

<!-- ```{r} -->
<!-- bank_train_X_lgb_ds = as.matrix(lgb.prepare_rules(bank_train_ds %>%  -->
<!--                                                     select(-y))[[1]]) -->
<!-- bank_test_X_lgb_ds = as.matrix(lgb.prepare_rules(bank_test_ds %>%  -->
<!--                                                    select(-y))[[1]]) -->
<!-- bank_train_Y_lgb_ds = as.matrix(bank_train_ds %>% -->
<!--                                   select(y) %>%  -->
<!--                                   mutate(y = as.numeric(as.character(y)))) -->
<!-- bank_test_Y_lgb_ds = as.matrix(bank_test_ds %>% -->
<!--                                  select(y) %>%  -->
<!--                                  mutate(y = as.numeric(as.character(y)))) -->

<!-- bank_train_lgb_ds = lgb.Dataset(bank_train_X_lgb_ds,  -->
<!--                                 label = bank_train_Y_lgb_ds) -->
<!-- bank_test_lgb_ds = lgb.Dataset(bank_test_X_lgb_ds,  -->
<!--                                label = bank_test_Y_lgb_ds) -->

<!-- params_lgb = list( -->
<!--   objective = "binary", # type of exercise -->
<!--   metric = "auc", # metric to be evaluated  -->
<!--   num_iterations = 500, # number of boosting iterations -->
<!--   early_stopping_rounds = 200, # ill stop training if one metric of one validation data doesn't improve -->
<!--   learning_rate = 0.1, # shrinkage rate -->
<!--   max_depth = 4, # max depth for tree model (used to deal with over-fitting when data is small) -->
<!--   num_leaves = 7, # max number of leaves (nodes) in one tree -->
<!--   # scale_pos_weight = (1 - table(bank_train_ds$y)[[2]]/length(bank_train_ds$y)) * 100, # weight for positive class -->
<!--   # is_unbalance = T, -->
<!--   min_data_in_leaf = 10, # min number of data in one leaf (used to deal with over-fitting) -->
<!--   feature_fraction = 0.9, # randomly select part of the features on each iteration -->
<!--   bagging_fraction = 0.9, # randomly select part of the data without resampling -->
<!--   bagging_freq = 1, # if != 0, enables bagging, performs bagging at every k iteration -->
<!--   num_threads = 6 # number of cpu cores (not threads) to use -->
<!-- ) -->

<!-- lgb_ds <- lgb.train( -->
<!--   params = params_lgb, -->
<!--   data = bank_train_lgb_ds, -->
<!--   valids = list(train = bank_train_lgb_ds,  -->
<!--                 test = bank_test_lgb_ds), -->
<!--   verbose = 1, # show results? -->
<!--   eval_freq = 50 # show metric every how many iterations? -->
<!-- ) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- lgb_train_score_ds = predict(lgb_ds, -->
<!--                              data = bank_train_X_lgb_ds, -->
<!--                              n = lgb_ds$best_iter) -->

<!-- lgb_test_score_ds = predict(lgb_ds, -->
<!--                             data = bank_test_X_lgb_ds, -->
<!--                             n = lgb_ds$best_iter) -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds = fun_gg_cutoff(lgb_train_score_ds, bank_train_Y_lgb_ds,  -->
<!--                                  "acc", "sens") -->
<!-- measure_train_ds + -->
<!--   geom_vline(xintercept = c(0.4, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->


<!-- #### Confusion matrix - train data -->


<!-- ```{r} -->
<!-- lgb_train_class_ds = fun_cut_predict(lgb_train_score_ds, 0.5) -->
<!-- lgb_train_confm_ds = confusionMatrix(lgb_train_class_ds, bank_train_ds$y,  -->
<!--                                      positive = "1") -->
<!-- lgb_train_confm_ds -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->


<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(lgb_test_score_ds, bank_test_Y_lgb_ds,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.4, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lgb_test_class_ds = fun_cut_predict(lgb_test_score_ds, 0.5) -->
<!-- lgb_test_confm_ds = confusionMatrix(lgb_test_class_ds, bank_test_ds$y,  -->
<!--                                     positive = "1") -->
<!-- lgb_test_confm_ds -->
<!-- ``` -->


<!-- ### XGBoost -->


<!-- ```{r} -->
<!-- bank_train_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_train_ds %>%  -->
<!--                                                     select(-y))[[1]]) -->
<!-- bank_test_X_xgb_ds = as.matrix(lgb.prepare_rules(bank_test_ds %>%  -->
<!--                                                    select(-y))[[1]]) -->
<!-- bank_train_Y_xgb_ds = as.matrix(bank_train_ds %>% -->
<!--                                   select(y) %>%  -->
<!--                                   mutate(y = as.numeric(as.character(y)))) -->
<!-- bank_test_Y_xgb_ds = as.matrix(bank_test_ds %>% -->
<!--                                  select(y) %>%  -->
<!--                                  mutate(y = as.numeric(as.character(y)))) -->

<!-- bank_train_xgb_ds = xgb.DMatrix(bank_train_X_xgb_ds,  -->
<!--                                 label = bank_train_Y_xgb_ds) -->
<!-- bank_test_xgb_ds = xgb.DMatrix(bank_test_X_xgb_ds,  -->
<!--                                label = bank_test_Y_xgb_ds) -->

<!-- params_xgb = list( -->
<!--   nthreads = 10, # parallel threads used -->
<!--   booster = "gbtree", -->
<!--   eval_metric = "auc", -->
<!--   learning_rate = 0.1, # shrinkage rate -->
<!--   max_depth = 4, # max depth for trees -->
<!--   subsample = 0.7, # subsample ratio per iteration -->
<!--   colsample_bytree = 0.7, # sample of features per iteration -->
<!--   min_child_weight = 5, -->
<!--   tree_method = "hist", # tree construction algorithm -->
<!--   max_bin = 10 # number of discrete bins to bucket continuous features -->
<!-- ) -->

<!-- xgb_ds <- xgb.train( -->
<!--   params = params_xgb, -->
<!--   data = bank_train_xgb_ds, -->
<!--   watchlist = list(train = bank_train_xgb_ds,  -->
<!--                    test = bank_test_xgb_ds), # test as second data for xvalidation purposes -->
<!--   nrounds = 1000, -->
<!--   verbose = T, -->
<!--   print_every_n = 25, -->
<!--   maximize = T, -->
<!--   early_stopping_rounds = 200 -->
<!-- ) -->
<!-- ``` -->

<!-- #### Predicted scores -->

<!-- ```{r} -->
<!-- xgb_train_score_ds = predict(xgb_ds, -->
<!--                              newdata = bank_train_X_xgb_ds, -->
<!--                              ntreelimit = xgb_ds$best_iteration) -->

<!-- xgb_test_score_ds = predict(xgb_ds, -->
<!--                             newdata = bank_test_X_xgb_ds, -->
<!--                             ntreelimit = xgb_ds$best_iteration) -->
<!-- ``` -->

<!-- #### Cut identification -->

<!-- ```{r} -->
<!-- measure_train_ds = fun_gg_cutoff(xgb_train_score_ds, bank_train_Y_xgb_ds,  -->
<!--                                  "acc", "sens") -->
<!-- measure_train_ds + -->
<!--   geom_vline(xintercept = c(0.4, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->


<!-- #### Confusion matrix - train data -->


<!-- ```{r} -->
<!-- xgb_train_class_ds = fun_cut_predict(xgb_train_score_ds, 0.4) -->
<!-- xgb_train_confm_ds = confusionMatrix(xgb_train_class_ds, bank_train_ds$y,  -->
<!--                                      positive = "1") -->
<!-- xgb_train_confm_ds -->
<!-- ``` -->

<!-- #### Validation (cut cost and confusion matrix) -->


<!-- ```{r} -->
<!-- measure_test_ds = fun_gg_cutoff(xgb_test_score_ds, bank_test_Y_xgb_ds,  -->
<!--                                 "acc", "sens") -->
<!-- measure_test_ds + -->
<!--   geom_vline(xintercept = c(0.75, 0.5),  -->
<!--              linetype = "dashed") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- xgb_test_class_ds = fun_cut_predict(xgb_test_score_ds, 0.75) -->
<!-- xgb_test_confm_ds = confusionMatrix(xgb_test_class_ds, bank_test_ds$y,  -->
<!--                                     positive = "1") -->
<!-- xgb_test_confm_ds -->
<!-- ``` -->










<!-- ### Model selection -->

<!-- #### ROC CURVE - train -->


<!-- ```{r} -->
<!-- score_train_ds = data.frame("logistic complex" = logistic_train_score_ds, -->
<!--                             "logistic simple" = logistic_train_score_ds_2, -->
<!--                             "tree complex" = tree_opt_train_score_ds, -->
<!--                             "tree simple" = tree_opt_train_score_ds_2, -->
<!--                             "random forest" = rf_train_score_ds, -->
<!--                             "light gbm" = lgb_train_score_ds, -->
<!--                             "xgboost" = xgb_train_score_ds, -->
<!--                             "obs" = as.numeric(bank_train_ds$y) - 1) %>% -->
<!--   gather(key = "Method", value = "score", -obs) -->

<!-- roc_train_ds = score_train_ds %>% -->
<!--   ggplot() + -->
<!--   aes(d = obs, -->
<!--       m = score, -->
<!--       color = Method) + -->
<!--   geom_roc() + -->
<!--   ggtitle("Train dataset") -->

<!-- print(roc_train_ds) -->
<!-- ``` -->



<!-- #### Prediction quality - train -->

<!-- ```{r, warning=F, message=F} -->
<!-- data.frame("Model" = c("Logistic regression (complex)", -->
<!--                        "Logistic regression (simple)", -->
<!--                        "Decision tree (complex)", -->
<!--                        "Decision tree (simple)", -->
<!--                        "Random forest (ranger)", -->
<!--                        "Light GBM", -->
<!--                        "XGBoost"), -->
<!--            "AUC" = c(auc(bank_train_ds$y, logistic_train_score_ds), -->
<!--                      auc(bank_train_ds$y, logistic_train_score_ds_2), -->
<!--                      auc(bank_train_ds$y, tree_opt_train_score_ds), -->
<!--                      auc(bank_train_ds$y, tree_opt_train_score_ds_2), -->
<!--                      auc(bank_train_ds$y, rf_train_score_ds), -->
<!--                      auc(bank_train_ds$y, lgb_train_score_ds), -->
<!--                      auc(bank_train_ds$y, xgb_train_score_ds)), -->
<!--            "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), -->
<!--            "Accuracy" = c(logistic_train_confm_ds[["overall"]][["Accuracy"]], -->
<!--                           logistic_train_confm_ds_2[["overall"]][["Accuracy"]], -->
<!--                           tree_opt_train_confm_ds[["overall"]][["Accuracy"]], -->
<!--                           tree_opt_train_confm_ds_2[["overall"]][["Accuracy"]], -->
<!--                           rf_train_confm_ds[["overall"]][["Accuracy"]], -->
<!--                           lgb_train_confm_ds[["overall"]][["Accuracy"]], -->
<!--                           xgb_train_confm_ds[["overall"]][["Accuracy"]]), -->
<!--            "Sensitivity" = c(logistic_train_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                              logistic_train_confm_ds_2[["byClass"]][["Sensitivity"]], -->
<!--                              tree_opt_train_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                              tree_opt_train_confm_ds_2[["byClass"]][["Sensitivity"]], -->
<!--                              rf_train_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                              lgb_train_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                              xgb_train_confm_ds[["byClass"]][["Sensitivity"]]), -->
<!--            stringsAsFactors = F) -->
<!-- ``` -->

<!-- #### ROC CURVE - validation -->


<!-- ```{r} -->
<!-- score_test_ds = data.frame("logistic (complex)" = logistic_test_score_ds, -->
<!--                            "logistic (simple)" = logistic_test_score_ds_2, -->
<!--                            "tree complex" = tree_opt_test_score_ds, -->
<!--                            "tree simple" = tree_opt_test_score_ds_2, -->
<!--                            "random forest" = rf_test_score_ds, -->
<!--                            "light gbm" = lgb_test_score_ds, -->
<!--                            "xgboost" = xgb_test_score_ds, -->
<!--                            "obs" = as.numeric(bank_test_ds$y) - 1) %>% -->
<!--   gather(key = "Method", value = "score", -obs) -->

<!-- roc_test_ds = score_test_ds %>% -->
<!--   ggplot() + -->
<!--   aes(d = obs, -->
<!--       m = score, -->
<!--       color = Method) + -->
<!--   geom_roc() + -->
<!--   ggtitle("Validation dataset") -->
<!-- roc_test_ds -->
<!-- ``` -->




<!-- #### Prediction quality - validation -->

<!-- ```{r, warning=F, message=F} -->
<!-- df_final_ds = data.frame("Model" = c("Logistic regression (complex)", -->
<!--                                      "Logistic regression (simple)", -->
<!--                                      "Decision tree (complex)", -->
<!--                                      "Decision tree (simple)", -->
<!--                                      "Random forest (ranger)", -->
<!--                                      "Light GBM", -->
<!--                                      "XGBoost"), -->
<!--                          "AUC" = c(auc(bank_test_ds$y, logistic_test_score_ds), -->
<!--                                    auc(bank_test_ds$y, logistic_test_score_ds_2), -->
<!--                                    auc(bank_test_ds$y, tree_opt_test_score_ds), -->
<!--                                    auc(bank_test_ds$y, tree_opt_test_score_ds_2), -->
<!--                                    auc(bank_test_ds$y, rf_test_score_ds), -->
<!--                                    auc(bank_test_ds$y, lgb_test_score_ds), -->
<!--                                    auc(bank_test_ds$y, xgb_test_score_ds)), -->
<!--                          "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), -->
<!--                          "Accuracy" = c(logistic_test_confm_ds[["overall"]][["Accuracy"]], -->
<!--                                         logistic_test_confm_ds_2[["overall"]][["Accuracy"]], -->
<!--                                         tree_opt_test_confm_ds[["overall"]][["Accuracy"]], -->
<!--                                         tree_opt_test_confm_ds_2[["overall"]][["Accuracy"]], -->
<!--                                         rf_test_confm_ds[["overall"]][["Accuracy"]], -->
<!--                                         lgb_test_confm_ds[["overall"]][["Accuracy"]], -->
<!--                                         xgb_test_confm_ds[["overall"]][["Accuracy"]]), -->
<!--                          "Sensitivity" = c(logistic_test_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                                            logistic_test_confm_ds_2[["byClass"]][["Sensitivity"]], -->
<!--                                            tree_opt_test_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                                            tree_opt_test_confm_ds_2[["byClass"]][["Sensitivity"]], -->
<!--                                            rf_test_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                                            lgb_test_confm_ds[["byClass"]][["Sensitivity"]], -->
<!--                                            xgb_test_confm_ds[["byClass"]][["Sensitivity"]]), -->
<!--                          stringsAsFactors = F) -->
<!-- df_final_ds -->
<!-- ``` -->


























## !!! CASE 3 - DOWNSAMPLING SAMPLING 

```{r}
set.seed(1234)

bank_train_us = upSample(x = bank_train %>% select(-y),
                         y = bank_train$y,
                         yname = "y")

bank_test_us = bank_test
```









### Logistic regression

The first model is a logistic regression, with every remaining features.

```{r}
logistic_us = glm(y ~ .,
                  data = bank_train_us,
                  family = "binomial")
```

#### Results

```{r}
summary(logistic_us)
```

A lot of features are non-significant in this model (including job, marital, education, previous, cons.price.idx and euribor3m). In a second time, we'll drop those features in order to build a more parsimonious model.

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_us)
```

In line with what was said just before, a lot of variables don't show enough importance to the model. This first regression model can be tuned to be more accurate.

#### Predicted scores

Let's obtain the predicted scores for both datasets. The training scores are useful to evaluate how well did the training go and the validation scores (test scores) will be used to validate the model (cross-validation).

```{r, message=F, warning=F}
logistic_train_score_us = predict(logistic_us,
                                  newdata = bank_train_us,
                                  type = "response")

logistic_test_score_us = predict(logistic_us,
                                 newdata = bank_test_us,
                                 type = "response")
```

#### Cut identification

Let's see if the default threshold (0.5) is appropriate with our scores. But first, we need to figure out which metric we should use to evaluate the model's performance.

Since we've unbalanced data, it's easy to reach a huge accuracy score. Indeed, even the idiotic model which predicts the prevalent group for every observation will have a nice accuracy score. What's interesting is to predict correctly customers that are actually willing to subscribe to a term-deposit (True positive). Contacting a customer that isn't ready to subscribe (False positive) to it is bad, but it isn't as bad as skipping a potential customer (False negative). This will still cost time and money to the banks. So we want to focus on True positive rate, while having a reasonably low false positive rate.

Recall is the ratio of the correctly positive labeled people among people who are willing to subscribe in reality and Precision is the ratio of the correctly positive labeled people among all the positive labeled people. We want to have high scores for both rates. The F1 score (or F score) is fitting perfectly to our situation. It's the harmonic average of the precision and recall. This is what we're going to focus on, while keeping an eye to the accuracy rate.

```{r}
measure_train_us = fun_gg_cutoff(logistic_train_score_us, bank_train_us$y, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

This plot shows the evolution of the accuracy and F1 rates according to the cut level. As it was said earlier, we want to have a good F1 score without dropping to much on accuracy. The 0.2 cut seems a good settlement, the trade-off between accuracy and F1 score is fair enough.

#### Confusion matrix - train data

Let's draw out the confusion matrix with the 0.2 cut.

```{r}
logistic_train_cut_us = 0.75
logistic_train_class_us = fun_cut_predict(logistic_train_score_us, logistic_train_cut_us)
# matrix
logistic_train_confm_us = confusionMatrix(logistic_train_class_us, bank_train_us$y, 
                                          positive = "1",
                                          mode = "everything")
logistic_train_confm_us
```

On the training set, the accuracy reachs 86.58% and the Sensitivity rate is close to 56.54%, which means that model manages to correctly label 86.58% of the times and 56.54% of the willingful customers are correctly detected. Let's evaluate this with the hold out data.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_us = fun_gg_cutoff(logistic_test_score_us, bank_test_us$y, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(logistic_train_cut_us, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_us = fun_cut_predict(logistic_test_score_us, logistic_train_cut_us)
# matrix
logistic_test_confm_us = confusionMatrix(logistic_test_class_us, bank_test_us$y, 
                                         positive = "1",
                                         mode = "everything")
logistic_test_confm_us
```

The performance values are close to the training ones, our model doesn't suffer from over-fitting.


Let's apply the same procedure with other models.
















### Logistic regression 2 (simple)

We've find out that some features weren't relevant to our model, so let's drop those and remake a logistic regression.

```{r}
logistic_us_2 = glm(y ~ . - euribor3m - cons.conf.idx,
                    data = bank_train_us,
                    family = "binomial")
```

#### Results

```{r}
summary(logistic_us_2)
```

This model seems as good as the previous one (according to the AIC), but with way lesser predictors.


#### Features importance

```{r}
fun_imp_ggplot_split(logistic_us_2)
```

Every feature is now revelant to the model.

#### Predicted scores

Same as before, we compute scores to evaluate and validate our model.

```{r}
logistic_train_score_us_2 = predict(logistic_us_2,
                                    newdata = bank_train_us,
                                    type = "response")

logistic_test_score_us_2 = predict(logistic_us_2,
                                   newdata = bank_test_us,
                                   type = "response")
```

#### Cut identification

```{r}
measure_train_us_2 = fun_gg_cutoff(logistic_train_score_us_2, bank_train_us$y, 
                                   "acc", "f")
measure_train_us_2 +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

The 0.2 threshold remains appropriate and is very close to the best cut for maximizing the F1 score.

#### Confusion matrix - train data

```{r}
logistic_train_cut_us_2 = 0.75
logistic_train_class_us_2 = fun_cut_predict(logistic_train_score_us_2, logistic_train_cut_us_2)
# matrix
logistic_train_confm_us_2 = confusionMatrix(logistic_train_class_us_2, bank_train_us$y, 
                                            positive = "1",
                                            mode = "everything")
logistic_train_confm_us_2
```

The accuracy slightly decreased but the sensitivity rate also slightly increased. Let's directly study the validation set.

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_us_2 = fun_gg_cutoff(logistic_test_score_us_2, bank_test_us$y, 
                                  "acc", "f")
measure_test_us_2 +
  geom_vline(xintercept = c(logistic_train_cut_us_2, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_us_2 = fun_cut_predict(logistic_test_score_us_2, logistic_train_cut_us_2)
# matrix
logistic_test_confm_us_2 = confusionMatrix(logistic_test_class_us_2, bank_test_us$y, 
                                           positive = "1",
                                           mode = "everything")
logistic_test_confm_us_2
```

At the end, the second logistic regression performs almost as good as the first model, but since we're seeking of high sensitivity rate, this little increase is welcome.






### Decision Tree

```{r}
tree_us = rpart(y ~ .,
                data = bank_train_us,
                cp = 0.0005)
```


```{r}
rpart.plot(tree_us)
```

Such a deep tree suffers from over-fitting, let's prune it according to its relative error evolution.

```{r}
plotcp(tree_us)
```

```{r}
cp_best_us = tree_us$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_us = prune(tree_us,
                    cp = cp_best_us)
```

```{r}
rpart.plot(tree_opt_us)
```

This is the pruned tree which has the lowest relative error.

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_us)
```

For the same reasons as before, we'll build another tree with lesser features in the next section.

#### Predicted scores

```{r}
tree_opt_train_score_us = predict(tree_opt_us,
                                  newdata = bank_train_us,
                                  type = "prob")[, 2]

tree_opt_test_score_us = predict(tree_opt_us,
                                 newdata = bank_test_us,
                                 type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(tree_opt_train_score_us, bank_train_us$y, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_us = 0.75
tree_opt_train_class_us = fun_cut_predict(tree_opt_train_score_us, tree_opt_train_cut_us)
tree_opt_train_confm_us = confusionMatrix(tree_opt_train_class_us, bank_train_us$y, 
                                          positive = "1",
                                          mode = "everything")
tree_opt_train_confm_us
```

The tree model is more accurate (around plus 2 percentage points) than the logistic ones on the training set, but its sensitivity has dropped (around minus 10 percentage points).

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(tree_opt_test_score_us, bank_test_us$y, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(tree_opt_train_cut_us, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_us = fun_cut_predict(tree_opt_test_score_us, tree_opt_train_cut_us)
# matrix
tree_opt_test_confm_us = confusionMatrix(tree_opt_test_class_us, bank_test_us$y, 
                                         positive = "1",
                                         mode = "everything")
tree_opt_test_confm_us
```

This decision tree suffers from over-fitting.






### Decision Tree 2

```{r}
tree_us_2 = rpart(y ~ euribor3m + nr.employed + cons.conf.idx + poutcome + pdays_dummy + month + cons.price.idx,
                  data = bank_train_us,
                  cp = 0.0005)
```


```{r}
rpart.plot(tree_us_2)
```

```{r}
plotcp(tree_us_2)
```

```{r}
cp_best_us_2 = tree_us_2$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_us_2 = prune(tree_us_2,
                      cp = cp_best_us_2)
```

```{r}
rpart.plot(tree_opt_us_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_us_2)
```

#### Predicted scores

```{r}
tree_opt_train_score_us_2 = predict(tree_opt_us_2,
                                    newdata = bank_train_us,
                                    type = "prob")[, 2]

tree_opt_test_score_us_2 = predict(tree_opt_us_2,
                                   newdata = bank_test_us,
                                   type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_us_2 = fun_gg_cutoff(tree_opt_train_score_us_2, bank_train_us$y, 
                                   "acc", "f")
measure_train_us_2 +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_cut_us_2 = 0.75
tree_opt_train_class_us_2 = fun_cut_predict(tree_opt_train_score_us_2, tree_opt_train_cut_us_2)
tree_opt_train_confm_us_2 = confusionMatrix(tree_opt_train_class_us_2, bank_train_us$y, 
                                            positive = "1",
                                            mode = "everything")
tree_opt_train_confm_us_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(tree_opt_test_score_us_2, bank_test_us$y, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(tree_opt_train_cut_us_2, 0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_us_2 = fun_cut_predict(tree_opt_test_score_us_2, tree_opt_train_cut_us_2)
# matrix
tree_opt_test_confm_us_2 = confusionMatrix(tree_opt_test_class_us_2, bank_test_us$y, 
                                           positive = "1",
                                           mode = "everything")
tree_opt_test_confm_us_2
```

This second decision tree is way better than the previous one, but the accuracy-sensitivity trade-off is delicate. We'll compare every models at the end.




### Random Forest (ranger)

```{r}
rf_us = ranger(y ~ .,
               data = bank_train_us,
               num.trees = 1000,
               importance = "impurity",
               write.forest = T,
               probability = T)
```


#### Results

```{r}
print(rf_us)
```

#### Features importance

```{r}
fun_imp_ggplot_split(rf_us)
```

#### Predicted scores

```{r}
rf_train_score_us = predict(rf_us,
                            data = bank_train_us)$predictions[, 2]

rf_test_score_us = predict(rf_us,
                           data = bank_test_us)$predictions[, 2]
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(rf_train_score_us, bank_train_us$y, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
rf_train_cut_us = 0.75
rf_train_class_us = fun_cut_predict(rf_train_score_us, rf_train_cut_us)
# matrix
rf_train_confm_us = confusionMatrix(rf_train_class_us, bank_train_us$y, 
                                    positive = "1",
                                    mode = "everything")
rf_train_confm_us
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(rf_test_score_us, bank_test_us$y, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(rf_train_cut_us, 0.5), 
             linetype = "dashed")
```

```{r}
rf_test_class_us = fun_cut_predict(rf_test_score_us, rf_train_cut_us)
# matrix
rf_test_confm_us = confusionMatrix(rf_test_class_us, bank_test_us$y, 
                                   positive = "1",
                                   mode = "everything")
rf_test_confm_us
```

### Light GBM

```{r}
bank_train_X_lgb_us = as.matrix(lgb.prepare_rules(bank_train_us %>% 
                                                    select(-y))[[1]])
bank_test_X_lgb_us = as.matrix(lgb.prepare_rules(bank_test_us %>% 
                                                   select(-y))[[1]])
bank_train_Y_lgb_us = as.matrix(bank_train_us %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_lgb_us = as.matrix(bank_test_us %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_lgb_us = lgb.Dataset(bank_train_X_lgb_us, 
                                label = bank_train_Y_lgb_us)
bank_test_lgb_us = lgb.Dataset(bank_test_X_lgb_us, 
                               label = bank_test_Y_lgb_us)

params_lgb = list(
  objective = "binary", # type of exercise
  metric = "auc", # metric to be evaluated 
  num_iterations = 500, # number of boosting iterations
  early_stopping_rounds = 200, # ill stop training if one metric of one validation data doesn't improve
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for tree model (used to deal with over-fitting when data is small)
  num_leaves = 7, # max number of leaves (nodes) in one tree
  # scale_pos_weight = (1 - table(bank_train_us$y)[[2]]/length(bank_train_us$y)) * 100, # weight for positive class
  is_unbalance = T,
  min_data_in_leaf = 10, # min number of data in one leaf (used to deal with over-fitting)
  feature_fraction = 0.9, # randomly select part of the features on each iteration
  bagging_fraction = 0.9, # randomly select part of the data without resampling
  bagging_freq = 1, # if != 0, enables bagging, performs bagging at every k iteration
  num_threads = 6 # number of cpu cores (not threads) to use
)

lgb_us <- lgb.train(
  params = params_lgb,
  data = bank_train_lgb_us,
  valids = list(train = bank_train_lgb_us, 
                test = bank_test_lgb_us),
  verbose = 1, # show results?
  eval_freq = 50 # show metric every how many iterations?
)
```

#### Predicted scores

```{r}
lgb_train_score_us = predict(lgb_us,
                             data = bank_train_X_lgb_us,
                             n = lgb_us$best_iter)

lgb_test_score_us = predict(lgb_us,
                            data = bank_test_X_lgb_us,
                            n = lgb_us$best_iter)
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(lgb_train_score_us, bank_train_Y_lgb_us, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
lgb_train_cut_us = 0.75
lgb_train_class_us = fun_cut_predict(lgb_train_score_us, lgb_train_cut_us)
lgb_train_confm_us = confusionMatrix(lgb_train_class_us, bank_train_us$y, 
                                     positive = "1",
                                     mode = "everything")
lgb_train_confm_us
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(lgb_test_score_us, bank_test_Y_lgb_us, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(lgb_train_cut_us, 0.5), 
             linetype = "dashed")
```

```{r}
lgb_test_class_us = fun_cut_predict(lgb_test_score_us, lgb_train_cut_us)
lgb_test_confm_us = confusionMatrix(lgb_test_class_us, bank_test_us$y, 
                                    positive = "1",
                                    mode = "everything")
lgb_test_confm_us
```


### XGBoost


```{r}
bank_train_X_xgb_us = as.matrix(lgb.prepare_rules(bank_train_us %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_us = as.matrix(lgb.prepare_rules(bank_test_us %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_us = as.matrix(bank_train_us %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_us = as.matrix(bank_test_us %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_us = xgb.DMatrix(bank_train_X_xgb_us, 
                                label = bank_train_Y_xgb_us)
bank_test_xgb_us = xgb.DMatrix(bank_test_X_xgb_us, 
                               label = bank_test_Y_xgb_us)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = 0.1, # shrinkage rate
  max_depth = 4, # max depth for trees
  subsample = 0.7, # subsample ratio per iteration
  colsample_bytree = 0.7, # sample of features per iteration
  min_child_weight = 5,
  tree_method = "hist", # tree construction algorithm
  max_bin = 10 # number of discrete bins to bucket continuous features
)

xgb_us <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_us,
  watchlist = list(train = bank_train_xgb_us, 
                   test = bank_test_xgb_us), # test as second data for xvalidation purposes
  nrounds = 1000,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = 200
)
```

#### Predicted scores

```{r}
xgb_train_score_us = predict(xgb_us,
                             newdata = bank_train_X_xgb_us,
                             ntreelimit = xgb_us$best_iteration)

xgb_test_score_us = predict(xgb_us,
                            newdata = bank_test_X_xgb_us,
                            ntreelimit = xgb_us$best_iteration)
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(xgb_train_score_us, bank_train_Y_xgb_us, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_us = 0.75
xgb_train_class_us = fun_cut_predict(xgb_train_score_us, xgb_train_cut_us)
xgb_train_confm_us = confusionMatrix(xgb_train_class_us, bank_train_us$y, 
                                     positive = "1",
                                     mode = "everything")
xgb_train_confm_us
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(xgb_test_score_us, bank_test_Y_xgb_us, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(xgb_train_cut_us, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_us = fun_cut_predict(xgb_test_score_us, xgb_train_cut_us)
xgb_test_confm_us = confusionMatrix(xgb_test_class_us, bank_test_us$y, 
                                    positive = "1",
                                    mode = "everything")
xgb_test_confm_us
```




### XGBoost hyperparameters search (caret)


```{r}
bank_X = as.matrix(lgb.prepare_rules(bank_data %>% 
                                       select(-y))[[1]])
bank_Y = bank_data$y

nrounds = 1000

tune_grid = expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control = trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  summaryFunction = prSummary,
  
  verboseIter = TRUE, # no training log
  allowParallel = FALSE # FALSE for reproducible results 
)

xgb_tune = train(
  x = bank_X,
  y = bank_Y,
  metric = "F",
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune) +
  theme(legend.position = "bottom")
```



```{r}
tune_grid2 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
                     c(xgb_tune$bestTune$max_depth:4),
                     (xgb_tune$bestTune$max_depth - 1):(xgb_tune$bestTune$max_depth + 1)),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune2) +
  theme(legend.position = "bottom")
```

```{r}
tune_grid3 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune3) +
  theme(legend.position = "bottom")
```


```{r, warning=F, message=F}
tune_grid4 = expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune4) +
  theme(legend.position = "bottom")
```


```{r}
tune_grid5 = expand.grid(
  nrounds = seq(from = 100, to = 2000, by = 100),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 = train(
  x = bank_X,
  y = bank_Y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  metric = "F",
  method = "xgbTree",
  verbose = TRUE
)

ggplot(xgb_tune5) +
  theme(legend.position = "bottom")
```


Now've every tuned hyperparameter.

```{r}
xgb_tune5$bestTune
```

Let's build our model.


```{r}
bank_train_X_xgb_us = as.matrix(lgb.prepare_rules(bank_train_us %>% 
                                                    select(-y))[[1]])
bank_test_X_xgb_us = as.matrix(lgb.prepare_rules(bank_test_us %>% 
                                                   select(-y))[[1]])
bank_train_Y_xgb_us = as.matrix(bank_train_us %>%
                                  select(y) %>% 
                                  mutate(y = as.numeric(as.character(y))))
bank_test_Y_xgb_us = as.matrix(bank_test_us %>%
                                 select(y) %>% 
                                 mutate(y = as.numeric(as.character(y))))

bank_train_xgb_us = xgb.DMatrix(bank_train_X_xgb_us, 
                                label = bank_train_Y_xgb_us)
bank_test_xgb_us = xgb.DMatrix(bank_test_X_xgb_us, 
                               label = bank_test_Y_xgb_us)

params_xgb = list(
  nthreads = 10, # parallel threads used
  booster = "gbtree",
  eval_metric = "aucpr",
  learning_rate = xgb_tune5$bestTune$eta, # shrinkage rate
  max_depth = xgb_tune5$bestTune$max_depth, # max depth for trees
  subsample = xgb_tune5$bestTune$subsample, # subsample ratio per iteration
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree, # sample of features per iteration
  min_child_weight = xgb_tune5$bestTune$min_child_weight
)

xgb_us_2 <- xgb.train(
  params = params_xgb,
  data = bank_train_xgb_us,
  watchlist = list(train = bank_train_xgb_us, 
                   test = bank_test_xgb_us), # test as second data for xvalidation purposes
  nrounds = xgb_tune5$bestTune$nrounds,
  verbose = T,
  print_every_n = 25,
  maximize = T,
  early_stopping_rounds = xgb_tune5$bestTune$nrounds
)
```

#### Predicted scores

```{r}
xgb_train_score_us_2 = predict(xgb_us_2,
                               newdata = bank_train_X_xgb_us,
                               ntreelimit = xgb_us_2$best_iteration)

xgb_test_score_us_2 = predict(xgb_us_2,
                              newdata = bank_test_X_xgb_us,
                              ntreelimit = xgb_us_2$best_iteration)
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(xgb_train_score_us_2, bank_train_Y_xgb_us, 
                                 "acc", "f")
measure_train_us +
  geom_vline(xintercept = c(0.75, 0.5), 
             linetype = "dashed")
```


#### Confusion matrix - train data


```{r}
xgb_train_cut_us_2 = 0.75
xgb_train_class_us_2 = fun_cut_predict(xgb_train_score_us_2, xgb_train_cut_us_2)
xgb_train_confm_us_2 = confusionMatrix(xgb_train_class_us_2, bank_train_us$y, 
                                       positive = "1",
                                       mode = "everything")
xgb_train_confm_us_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(xgb_test_score_us_2, bank_test_Y_xgb_us, 
                                "acc", "f")
measure_test_us +
  geom_vline(xintercept = c(xgb_train_cut_us_2, 0.5), 
             linetype = "dashed")
```

```{r}
xgb_test_class_us_2 = fun_cut_predict(xgb_test_score_us_2, xgb_train_cut_us_2)
xgb_test_confm_us_2 = confusionMatrix(xgb_test_class_us_2, bank_test_us$y, 
                                      positive = "1",
                                      mode = "everything")
xgb_test_confm_us_2
```


### Model selection

#### ROC and PR curves - train


```{r}
score_train_us = data.frame("logistic complex" = logistic_train_score_us,
                            "logistic simple" = logistic_train_score_us_2,
                            "tree complex" = tree_opt_train_score_us,
                            "tree simple" = tree_opt_train_score_us_2,
                            "random forest" = rf_train_score_us,
                            "light gbm" = lgb_train_score_us,
                            "xgboost default" = xgb_train_score_us,
                            "xgboost tuned" = xgb_train_score_us_2,
                            "obs" = as.numeric(bank_train_us$y) - 1)


roc_train_us = score_train_us %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Train dataset")

prcurve_train_us = gg_prcurve(score_train_us) + ggtitle("PR Curve", subtitle = "Train dataset")

curves_train_us = ggarrange(roc_train_us, prcurve_train_us, 
                            common.legend = T,
                            legend = "bottom")
```

```{r}
print(curves_train_us)
```


#### Prediction quality - train

```{r, warning=F, message=F}
data.frame("Model" = c("Logistic regression (complex)",
                       "Logistic regression (simple)",
                       "Decision tree (complex)",
                       "Decision tree (simple)",
                       "Random forest (ranger)",
                       "Light GBM (default)",
                       "XGBoost (default)",
                       "XGBoost (tuned)"),
           "AUROC" = c(auc(bank_train_us$y, logistic_train_score_us),
                       auc(bank_train_us$y, logistic_train_score_us_2),
                       auc(bank_train_us$y, tree_opt_train_score_us),
                       auc(bank_train_us$y, tree_opt_train_score_us_2),
                       auc(bank_train_us$y, rf_train_score_us),
                       auc(bank_train_us$y, lgb_train_score_us),
                       auc(bank_train_us$y, xgb_train_score_us),
                       auc(bank_train_us$y, xgb_train_score_us_2)),
           "AUPR" = c(aucpr(bank_train_us$y, logistic_train_score_us),
                      aucpr(bank_train_us$y, logistic_train_score_us_2),
                      aucpr(bank_train_us$y, tree_opt_train_score_us),
                      aucpr(bank_train_us$y, tree_opt_train_score_us_2),
                      aucpr(bank_train_us$y, rf_train_score_us),
                      aucpr(bank_train_us$y, lgb_train_score_us),
                      aucpr(bank_train_us$y, xgb_train_score_us),
                      aucpr(bank_train_us$y, xgb_train_score_us_2)),
           "Cut" = c(logistic_train_cut_us,
                     logistic_train_cut_us_2,
                     tree_opt_train_cut_us,
                     tree_opt_train_cut_us_2,
                     rf_train_cut_us,
                     lgb_train_cut_us,
                     xgb_train_cut_us,
                     xgb_train_cut_us_2),
           "Accuracy" = c(logistic_train_confm_us[["overall"]][["Accuracy"]],
                          logistic_train_confm_us_2[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_us[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_us_2[["overall"]][["Accuracy"]],
                          rf_train_confm_us[["overall"]][["Accuracy"]],
                          lgb_train_confm_us[["overall"]][["Accuracy"]],
                          xgb_train_confm_us[["overall"]][["Accuracy"]],
                          xgb_train_confm_us_2[["overall"]][["Accuracy"]]),
           "F1" = c(logistic_train_confm_us[["byClass"]][["F1"]],
                    logistic_train_confm_us_2[["byClass"]][["F1"]],
                    tree_opt_train_confm_us[["byClass"]][["F1"]],
                    tree_opt_train_confm_us_2[["byClass"]][["F1"]],
                    rf_train_confm_us[["byClass"]][["F1"]],
                    lgb_train_confm_us[["byClass"]][["F1"]],
                    xgb_train_confm_us[["byClass"]][["F1"]],
                    xgb_train_confm_us_2[["byClass"]][["F1"]]),
           stringsAsFactors = F)
```

#### ROC CURVE - validation

```{r}
score_test_us = data.frame("logistic complex" = logistic_test_score_us,
                           "logistic simple" = logistic_test_score_us_2,
                           "tree complex" = tree_opt_test_score_us,
                           "tree simple" = tree_opt_test_score_us_2,
                           "random forest" = rf_test_score_us,
                           "light gbm" = lgb_test_score_us,
                           "xgboost default" = xgb_test_score_us,
                           "xgboost tuned" = xgb_test_score_us_2,
                           "obs" = as.numeric(bank_test_us$y) - 1)


roc_test_us = score_test_us %>%
  gather(key = "Method", value = "score", -obs) %>% 
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc(labels = F, pointsize = 0, size = 0.6) +
  xlab("Specificity") +
  ylab("Sensitivity") +
  ggtitle("ROC Curve", subtitle = "Validation dataset")

prcurve_test_us = gg_prcurve(score_test_us) + ggtitle("PR Curve", subtitle = "Validation dataset")

curves_test_us = ggarrange(roc_test_us, prcurve_test_us, 
                           common.legend = T,
                           legend = "bottom")
```

```{r}
print(curves_test_us)
```




#### Prediction quality - validation

```{r, warning=F, message=F}
df_final_us = data.frame("Model" = c("Logistic regression (complex)",
                                     "Logistic regression (simple)",
                                     "Decision tree (complex)",
                                     "Decision tree (simple)",
                                     "Random forest (ranger)",
                                     "Light GBM (default)",
                                     "XGBoost (default)",
                                     "XGBoost (tuned)"),
                         "AUROC" = c(auc(bank_test_us$y, logistic_test_score_us),
                                     auc(bank_test_us$y, logistic_test_score_us_2),
                                     auc(bank_test_us$y, tree_opt_test_score_us),
                                     auc(bank_test_us$y, tree_opt_test_score_us_2),
                                     auc(bank_test_us$y, rf_test_score_us),
                                     auc(bank_test_us$y, lgb_test_score_us),
                                     auc(bank_test_us$y, xgb_test_score_us),
                                     auc(bank_test_us$y, xgb_test_score_us_2)),
                         "AUPR" = c(aucpr(bank_test_us$y, logistic_test_score_us),
                                    aucpr(bank_test_us$y, logistic_test_score_us_2),
                                    aucpr(bank_test_us$y, tree_opt_test_score_us),
                                    aucpr(bank_test_us$y, tree_opt_test_score_us_2),
                                    aucpr(bank_test_us$y, rf_test_score_us),
                                    aucpr(bank_test_us$y, lgb_test_score_us),
                                    aucpr(bank_test_us$y, xgb_test_score_us),
                                    aucpr(bank_test_us$y, xgb_test_score_us_2)),
                         "Cut" = c(logistic_train_cut_us,
                                   logistic_train_cut_us_2,
                                   tree_opt_train_cut_us,
                                   tree_opt_train_cut_us_2,
                                   rf_train_cut_us,
                                   lgb_train_cut_us,
                                   xgb_train_cut_us,
                                   xgb_train_cut_us_2),
                         "Accuracy" = c(logistic_test_confm_us[["overall"]][["Accuracy"]],
                                        logistic_test_confm_us_2[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_us[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_us_2[["overall"]][["Accuracy"]],
                                        rf_test_confm_us[["overall"]][["Accuracy"]],
                                        lgb_test_confm_us[["overall"]][["Accuracy"]],
                                        xgb_test_confm_us[["overall"]][["Accuracy"]],
                                        xgb_test_confm_us_2[["overall"]][["Accuracy"]]),
                         "F1" = c(logistic_test_confm_us[["byClass"]][["F1"]],
                                  logistic_test_confm_us_2[["byClass"]][["F1"]],
                                  tree_opt_test_confm_us[["byClass"]][["F1"]],
                                  tree_opt_test_confm_us_2[["byClass"]][["F1"]],
                                  rf_test_confm_us[["byClass"]][["F1"]],
                                  lgb_test_confm_us[["byClass"]][["F1"]],
                                  xgb_test_confm_us[["byClass"]][["F1"]],
                                  xgb_test_confm_us_2[["byClass"]][["F1"]]),
                         stringsAsFactors = F)
df_final_us %>% 
  arrange(-Accuracy, -F1)
```










































### Logistic regression

```{r}
logistic_us = glm(y ~ .,
                  data = bank_train_us,
                  family = "binomial")
```

#### Results

```{r}
summary(logistic_us)
```

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_us)
```

#### Predicted scores

```{r}
logistic_train_score_us = predict(logistic_us,
                                  newdata = bank_train_us,
                                  type = "response")

logistic_test_score_us = predict(logistic_us,
                                 newdata = bank_test_us,
                                 type = "response")
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(logistic_train_score_us, bank_train_us$y, 
                                 "acc", "sens")
measure_train_us +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
logistic_train_class_us = fun_cut_predict(logistic_train_score_us, 0.5)
# matrix
logistic_train_confm_us = confusionMatrix(logistic_train_class_us, bank_train_us$y, 
                                          positive = "1")
logistic_train_confm_us
```

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_us = fun_gg_cutoff(logistic_test_score_us, bank_test_us$y, 
                                "acc", "sens")
measure_test_us +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_us = fun_cut_predict(logistic_test_score_us, 0.5)
# matrix
logistic_test_confm_us = confusionMatrix(logistic_test_class_us, bank_test_us$y, 
                                         positive = "1")
logistic_test_confm_us
```




### Logistic regression 2 (simple)

```{r}
logistic_us_2 = glm(y ~ . - cons.price.idx - euribor3m,
                    data = bank_train_us,
                    family = "binomial")
```

#### Results

```{r}
summary(logistic_us_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_us_2)
```

#### Predicted scores

```{r}
logistic_train_score_us_2 = predict(logistic_us_2,
                                    newdata = bank_train_us,
                                    type = "response")

logistic_test_score_us_2 = predict(logistic_us_2,
                                   newdata = bank_test_us,
                                   type = "response")
```

#### Cut identification

```{r}
measure_train_us_2 = fun_gg_cutoff(logistic_train_score_us_2, bank_train_us$y, 
                                   "acc", "sens")
measure_train_us_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
logistic_train_class_us_2 = fun_cut_predict(logistic_train_score_us_2, 0.5)
# matrix
logistic_train_confm_us_2 = confusionMatrix(logistic_train_class_us_2, bank_train_us$y, 
                                            positive = "1")
logistic_train_confm_us_2
```

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_us_2 = fun_gg_cutoff(logistic_test_score_us_2, bank_test_us$y, 
                                  "acc", "sens")
measure_test_us_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_us_2 = fun_cut_predict(logistic_test_score_us_2, 0.5)
# matrix
logistic_test_confm_us_2 = confusionMatrix(logistic_test_class_us_2, bank_test_us$y, 
                                           positive = "1")
logistic_test_confm_us_2
```





### Decision Tree

```{r}
tree_us = rpart(y ~ .,
                data = bank_train_us,
                cp = 0.0005)
```


```{r}
rpart.plot(tree_us)
```

```{r}
plotcp(tree_us)
```

```{r}
cp_best_us = tree_us$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_us = prune(tree_us,
                    cp = cp_best_us)
```

```{r}
rpart.plot(tree_opt_us)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_us)
```

#### Predicted scores

```{r}
tree_opt_train_score_us = predict(tree_opt_us,
                                  newdata = bank_train_us,
                                  type = "prob")[, 2]

tree_opt_test_score_us = predict(tree_opt_us,
                                 newdata = bank_test_us,
                                 type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(tree_opt_train_score_us, bank_train_us$y, 
                                 "acc", "sens")
measure_train_us +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_class_us = fun_cut_predict(tree_opt_train_score_us, 0.5)
tree_opt_train_confm_us = confusionMatrix(tree_opt_train_class_us, bank_train_us$y, 
                                          positive = "1")
tree_opt_train_confm_us
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(tree_opt_test_score_us, bank_test_us$y, 
                                "acc", "sens")
measure_test_us +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_us = fun_cut_predict(tree_opt_test_score_us, 0.5)
# matrix
tree_opt_test_confm_us = confusionMatrix(tree_opt_test_class_us, bank_test_us$y, 
                                         positive = "1")
tree_opt_test_confm_us
```








### Decision Tree 2

```{r}
tree_us_2 = rpart(y ~ nr.employed + euribor3m + cons.conf.idx + pdays_dummy + poutcome + cons.price.idx + month,
                  data = bank_train_us,
                  cp = 0.0005)
```


```{r}
rpart.plot(tree_us_2)
```

```{r}
plotcp(tree_us_2)
```

```{r}
cp_best_us_2 = tree_us_2$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_us_2 = prune(tree_us_2,
                      cp = cp_best_us_2)
```

```{r}
rpart.plot(tree_opt_us_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_us_2)
```

#### Predicted scores

```{r}
tree_opt_train_score_us_2 = predict(tree_opt_us_2,
                                    newdata = bank_train_us,
                                    type = "prob")[, 2]

tree_opt_test_score_us_2 = predict(tree_opt_us_2,
                                   newdata = bank_test_us,
                                   type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_us_2 = fun_gg_cutoff(tree_opt_train_score_us_2, bank_train_us$y, 
                                   "acc", "sens")
measure_train_us_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_class_us_2 = fun_cut_predict(tree_opt_train_score_us_2, 0.5)
tree_opt_train_confm_us_2 = confusionMatrix(tree_opt_train_class_us_2, bank_train_us$y, 
                                            positive = "1")
tree_opt_train_confm_us_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(tree_opt_test_score_us_2, bank_test_us$y, 
                                "acc", "sens")
measure_test_us +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_us_2 = fun_cut_predict(tree_opt_test_score_us_2, 0.5)
# matrix
tree_opt_test_confm_us_2 = confusionMatrix(tree_opt_test_class_us_2, bank_test_us$y, 
                                           positive = "1")
tree_opt_test_confm_us_2
```






### Random Forest (ranger)

```{r}
rf_us = ranger(y ~ .,
               data = bank_train_us,
               num.trees = 1000,
               importance = "impurity",
               write.forest = T,
               probability = T)
```


#### Results

```{r}
print(rf_us)
```

#### Features importance

```{r}
fun_imp_ggplot_split(rf_us)
```

#### Predicted scores

```{r}
rf_train_score_us = predict(rf_us,
                            data = bank_train_us)$predictions[, 2]

rf_test_score_us = predict(rf_us,
                           data = bank_test_us)$predictions[, 2]
```

#### Cut identification

```{r}
measure_train_us = fun_gg_cutoff(rf_train_score_us, bank_train_us$y, 
                                 "acc", "sens")
measure_train_us +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
rf_train_class_us = fun_cut_predict(rf_train_score_us, 0.375)
# matrix
rf_train_confm_us = confusionMatrix(rf_train_class_us, bank_train_us$y, 
                                    positive = "1")
rf_train_confm_us
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_us = fun_gg_cutoff(rf_test_score_us, bank_test_us$y, 
                                "acc", "sens")
measure_test_us +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

```{r}
rf_test_class_us = fun_cut_predict(rf_test_score_us, 0.5)
# matrix
rf_test_confm_us = confusionMatrix(rf_test_class_us, bank_test_us$y, 
                                   positive = "1")
rf_test_confm_us
```

### Model selection

#### ROC CURVE - train


```{r}
score_train_us = data.frame("logistic complex" = logistic_train_score_us,
                            "logistic simple" = logistic_train_score_us_2,
                            "tree complex" = tree_opt_train_score_us,
                            "tree simple" = tree_opt_train_score_us_2,
                            "random forest" = rf_train_score_us,
                            "obs" = as.numeric(bank_train_us$y) - 1) %>%
  gather(key = "Method", value = "score", -obs)

roc_train_us = score_train_us %>%
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc() +
  ggtitle("Train dataset")

print(roc_train_us)
```



#### Prediction quality - train

```{r, warning=F, message=F}
data.frame("Model" = c("Logistic regression (complex)",
                       "Logistic regression (simple)",
                       "Decision tree (complex)",
                       "Decision tree (simple)",
                       "Random forest (ranger)"),
           "AUC" = c(auc(bank_train_us$y, logistic_train_score_us),
                     auc(bank_train_us$y, logistic_train_score_us_2),
                     auc(bank_train_us$y, tree_opt_train_score_us),
                     auc(bank_train_us$y, tree_opt_train_score_us_2),
                     auc(bank_train_us$y, rf_train_score_us)),
           "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5),
           "Accuracy" = c(logistic_train_confm_us[["overall"]][["Accuracy"]],
                          logistic_train_confm_us_2[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_us[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_us_2[["overall"]][["Accuracy"]],
                          rf_train_confm_us[["overall"]][["Accuracy"]]),
           "Sensitivity" = c(logistic_train_confm_us[["byClass"]][["Sensitivity"]],
                             logistic_train_confm_us_2[["byClass"]][["Sensitivity"]],
                             tree_opt_train_confm_us[["byClass"]][["Sensitivity"]],
                             tree_opt_train_confm_us_2[["byClass"]][["Sensitivity"]],
                             rf_train_confm_us[["byClass"]][["Sensitivity"]]),
           stringsAsFactors = F)
```

#### ROC CURVE - validation


```{r}
score_test_us = data.frame("logistic (complex)" = logistic_test_score_us,
                           "logistic (simple)" = logistic_test_score_us_2,
                           "tree complex" = tree_opt_test_score_us,
                           "tree simple" = tree_opt_test_score_us_2,
                           "random forest" = rf_test_score_us,
                           "obs" = as.numeric(bank_test_us$y) - 1) %>%
  gather(key = "Method", value = "score", -obs)

roc_test_us = score_test_us %>%
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc() +
  ggtitle("Validation dataset")
roc_test_us
```




#### Prediction quality - validation

```{r, warning=F, message=F}
df_final_us = data.frame("Model" = c("Logistic regression (complex)",
                                     "Logistic regression (simple)",
                                     "Decision tree (complex)",
                                     "Decision tree (simple)",
                                     "Random forest (ranger)"),
                         "AUC" = c(auc(bank_test_us$y, logistic_test_score_us),
                                   auc(bank_test_us$y, logistic_test_score_us_2),
                                   auc(bank_test_us$y, tree_opt_test_score_us),
                                   auc(bank_test_us$y, tree_opt_test_score_us_2),
                                   auc(bank_test_us$y, rf_test_score_us)),
                         "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5),
                         "Accuracy" = c(logistic_test_confm_us[["overall"]][["Accuracy"]],
                                        logistic_test_confm_us_2[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_us[["overall"]][["Accuracy"]],
                                        tree_opt_test_confm_us_2[["overall"]][["Accuracy"]],
                                        rf_test_confm_us[["overall"]][["Accuracy"]]),
                         "Sensitivity" = c(logistic_test_confm_us[["byClass"]][["Sensitivity"]],
                                           logistic_test_confm_us_2[["byClass"]][["Sensitivity"]],
                                           tree_opt_test_confm_us[["byClass"]][["Sensitivity"]],
                                           tree_opt_test_confm_us_2[["byClass"]][["Sensitivity"]],
                                           rf_test_confm_us[["byClass"]][["Sensitivity"]]),
                         stringsAsFactors = F)
df_final_us
```
































## !!! CASE 4 - SMOTE SAMPLING 

```{r}
set.seed(1234)

bank_train_smote = SMOTE(y ~ .,
                         data = bank_train)

bank_test_smote = bank_test
```


### Logistic regression

```{r}
logistic_smote = glm(y ~ .,
                     data = bank_train_smote,
                     family = "binomial")
```

#### Results

```{r}
summary(logistic_smote)
```

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_smote)
```

#### Predicted scores

```{r}
logistic_train_score_smote = predict(logistic_smote,
                                     newdata = bank_train_smote,
                                     type = "response")

logistic_test_score_smote = predict(logistic_smote,
                                    newdata = bank_test_smote,
                                    type = "response")
```

#### Cut identification

```{r}
measure_train_smote = fun_gg_cutoff(logistic_train_score_smote, bank_train_smote$y, 
                                    "acc", "sens")
measure_train_smote +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
logistic_train_class_smote = fun_cut_predict(logistic_train_score_smote, 0.5)
# matrix
logistic_train_confm_smote = confusionMatrix(logistic_train_class_smote, bank_train_smote$y, 
                                             positive = "1")
logistic_train_confm_smote
```

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_smote = fun_gg_cutoff(logistic_test_score_smote, bank_test_smote$y, 
                                   "acc", "sens")
measure_test_smote +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_smote = fun_cut_predict(logistic_test_score_smote, 0.5)
# matrix
logistic_test_confm_smote = confusionMatrix(logistic_test_class_smote, bank_test_smote$y, 
                                            positive = "1")
logistic_test_confm_smote
```




### Logistic regression 2 (simple)

```{r}
logistic_smote_2 = glm(y ~ . - education - euribor3m - cons.conf.idx,
                       data = bank_train_smote,
                       family = "binomial")
```

#### Results

```{r}
summary(logistic_smote_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(logistic_smote_2)
```

#### Predicted scores

```{r}
logistic_train_score_smote_2 = predict(logistic_smote_2,
                                       newdata = bank_train_smote,
                                       type = "response")

logistic_test_score_smote_2 = predict(logistic_smote_2,
                                      newdata = bank_test_smote,
                                      type = "response")
```

#### Cut identification

```{r}
measure_train_smote_2 = fun_gg_cutoff(logistic_train_score_smote_2, bank_train_smote$y, 
                                      "acc", "sens")
measure_train_smote_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
logistic_train_class_smote_2 = fun_cut_predict(logistic_train_score_smote_2, 0.5)
# matrix
logistic_train_confm_smote_2 = confusionMatrix(logistic_train_class_smote_2, bank_train_smote$y, 
                                               positive = "1")
logistic_train_confm_smote_2
```

#### Validation (cut cost and confusion matrix)

```{r}
measure_test_smote_2 = fun_gg_cutoff(logistic_test_score_smote_2, bank_test_smote$y, 
                                     "acc", "sens")
measure_test_smote_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
logistic_test_class_smote_2 = fun_cut_predict(logistic_test_score_smote_2, 0.5)
# matrix
logistic_test_confm_smote_2 = confusionMatrix(logistic_test_class_smote_2, bank_test_smote$y, 
                                              positive = "1")
logistic_test_confm_smote_2
```




### Decision Tree

```{r}
tree_smote = rpart(y ~ .,
                   data = bank_train_smote,
                   cp = 0.0005)
```


```{r}
rpart.plot(tree_smote)
```

```{r}
plotcp(tree_smote)
```

```{r}
cp_best_smote = tree_smote$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_smote = prune(tree_smote,
                       cp = cp_best_smote)
```

```{r}
rpart.plot(tree_opt_smote)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_smote)
```

#### Predicted scores

```{r}
tree_opt_train_score_smote = predict(tree_opt_smote,
                                     newdata = bank_train_smote,
                                     type = "prob")[, 2]

tree_opt_test_score_smote = predict(tree_opt_smote,
                                    newdata = bank_test_smote,
                                    type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_smote = fun_gg_cutoff(tree_opt_train_score_smote, bank_train_smote$y, 
                                    "acc", "sens")
measure_train_smote +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_class_smote = fun_cut_predict(tree_opt_train_score_smote, 0.5)
tree_opt_train_confm_smote = confusionMatrix(tree_opt_train_class_smote, bank_train_smote$y, 
                                             positive = "1")
tree_opt_train_confm_smote
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_smote = fun_gg_cutoff(tree_opt_test_score_smote, bank_test_smote$y, 
                                   "acc", "sens")
measure_test_smote +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_smote = fun_cut_predict(tree_opt_test_score_smote, 0.5)
# matrix
tree_opt_test_confm_smote = confusionMatrix(tree_opt_test_class_smote, bank_test_smote$y, 
                                            positive = "1")
tree_opt_test_confm_smote
```








### Decision Tree 2

```{r}
tree_smote_2 = rpart(y ~ nr.employed + euribor3m + cons.conf.idx + pdays_dummy + 
                       poutcome + cons.price.idx + month + previous,
                     data = bank_train_smote,
                     cp = 0.0005)
```


```{r}
rpart.plot(tree_smote_2)
```

```{r}
plotcp(tree_smote_2)
```

```{r}
cp_best_smote_2 = tree_smote_2$cptable %>%
  as.data.frame %>%
  filter(xerror == min(xerror)) %>%
  select(CP) %>%
  slice(1) %>%
  as.numeric()

tree_opt_smote_2 = prune(tree_smote_2,
                         cp = cp_best_smote_2)
```

```{r}
rpart.plot(tree_opt_smote_2)
```

#### Features importance

```{r}
fun_imp_ggplot_split(tree_opt_smote_2)
```

#### Predicted scores

```{r}
tree_opt_train_score_smote_2 = predict(tree_opt_smote_2,
                                       newdata = bank_train_smote,
                                       type = "prob")[, 2]

tree_opt_test_score_smote_2 = predict(tree_opt_smote_2,
                                      newdata = bank_test_smote,
                                      type = "prob")[, 2]
```

#### Cut identification

```{r}
measure_train_smote_2 = fun_gg_cutoff(tree_opt_train_score_smote_2, bank_train_smote$y, 
                                      "acc", "sens")
measure_train_smote_2 +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data


```{r}
tree_opt_train_class_smote_2 = fun_cut_predict(tree_opt_train_score_smote_2, 0.5)
tree_opt_train_confm_smote_2 = confusionMatrix(tree_opt_train_class_smote_2, bank_train_smote$y, 
                                               positive = "1")
tree_opt_train_confm_smote_2
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_smote = fun_gg_cutoff(tree_opt_test_score_smote_2, bank_test_smote$y, 
                                   "acc", "sens")
measure_test_smote +
  geom_vline(xintercept = c(0.5), 
             linetype = "dashed")
```

```{r}
tree_opt_test_class_smote_2 = fun_cut_predict(tree_opt_test_score_smote_2, 0.5)
# matrix
tree_opt_test_confm_smote_2 = confusionMatrix(tree_opt_test_class_smote_2, bank_test_smote$y, 
                                              positive = "1")
tree_opt_test_confm_smote_2
```






### Random Forest (ranger)

```{r}
rf_smote = ranger(y ~ .,
                  data = bank_train_smote,
                  num.trees = 1000,
                  importance = "impurity",
                  write.forest = T,
                  probability = T)
```


#### Results

```{r}
print(rf_smote)
```

#### Features importance

```{r}
fun_imp_ggplot_split(rf_smote)
```

#### Predicted scores

```{r}
rf_train_score_smote = predict(rf_smote,
                               data = bank_train_smote)$predictions[, 2]

rf_test_score_smote = predict(rf_smote,
                              data = bank_test_smote)$predictions[, 2]
```

#### Cut identification

```{r}
measure_train_smote = fun_gg_cutoff(rf_train_score_smote, bank_train_smote$y, 
                                    "acc", "sens")
measure_train_smote +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

#### Confusion matrix - train data

```{r}
rf_train_class_smote = fun_cut_predict(rf_train_score_smote, 0.375)
# matrix
rf_train_confm_smote = confusionMatrix(rf_train_class_smote, bank_train_smote$y, 
                                       positive = "1")
rf_train_confm_smote
```

#### Validation (cut cost and confusion matrix)


```{r}
measure_test_smote = fun_gg_cutoff(rf_test_score_smote, bank_test_smote$y, 
                                   "acc", "sens")
measure_test_smote +
  geom_vline(xintercept = c(0.375, 0.5), 
             linetype = "dashed")
```

```{r}
rf_test_class_smote = fun_cut_predict(rf_test_score_smote, 0.5)
# matrix
rf_test_confm_smote = confusionMatrix(rf_test_class_smote, bank_test_smote$y, 
                                      positive = "1")
rf_test_confm_smote
```

### Model selection

#### ROC CURVE - train


```{r}
score_train_smote = data.frame("logistic complex" = logistic_train_score_smote,
                               "logistic simple" = logistic_train_score_smote_2,
                               "tree complex" = tree_opt_train_score_smote,
                               "tree simple" = tree_opt_train_score_smote_2,
                               "random forest" = rf_train_score_smote,
                               "obs" = as.numeric(bank_train_smote$y) - 1) %>%
  gather(key = "Method", value = "score", -obs)

roc_train_smote = score_train_smote %>%
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc() +
  ggtitle("Train dataset")

print(roc_train_smote)
```



#### Prediction quality - train

```{r, warning=F, message=F}
data.frame("Model" = c("Logistic regression (complex)",
                       "Logistic regression (simple)",
                       "Decision tree (complex)",
                       "Decision tree (simple)",
                       "Random forest (ranger)"),
           "AUC" = c(auc(bank_train_smote$y, logistic_train_score_smote),
                     auc(bank_train_smote$y, logistic_train_score_smote_2),
                     auc(bank_train_smote$y, tree_opt_train_score_smote),
                     auc(bank_train_smote$y, tree_opt_train_score_smote_2),
                     auc(bank_train_smote$y, rf_train_score_smote)),
           "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5),
           "Accuracy" = c(logistic_train_confm_smote[["overall"]][["Accuracy"]],
                          logistic_train_confm_smote_2[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_smote[["overall"]][["Accuracy"]],
                          tree_opt_train_confm_smote_2[["overall"]][["Accuracy"]],
                          rf_train_confm_smote[["overall"]][["Accuracy"]]),
           "Sensitivity" = c(logistic_train_confm_smote[["byClass"]][["Sensitivity"]],
                             logistic_train_confm_smote_2[["byClass"]][["Sensitivity"]],
                             tree_opt_train_confm_smote[["byClass"]][["Sensitivity"]],
                             tree_opt_train_confm_smote_2[["byClass"]][["Sensitivity"]],
                             rf_train_confm_smote[["byClass"]][["Sensitivity"]]),
           stringsAsFactors = F)
```

#### ROC CURVE - validation


```{r}
score_test_smote = data.frame("logistic (complex)" = logistic_test_score_smote,
                              "logistic (simple)" = logistic_test_score_smote_2,
                              "tree complex" = tree_opt_test_score_smote,
                              "tree simple" = tree_opt_test_score_smote_2,
                              "random forest" = rf_test_score_smote,
                              "obs" = as.numeric(bank_test_smote$y) - 1) %>%
  gather(key = "Method", value = "score", -obs)

roc_test_smote = score_test_smote %>%
  ggplot() +
  aes(d = obs,
      m = score,
      color = Method) +
  geom_roc() +
  ggtitle("Validation dataset")
roc_test_smote
```




#### Prediction quality - validation

```{r, warning=F, message=F}
df_final_smote = data.frame("Model" = c("Logistic regression (complex)",
                                        "Logistic regression (simple)",
                                        "Decision tree (complex)",
                                        "Decision tree (simple)",
                                        "Random forest (ranger)"),
                            "AUC" = c(auc(bank_test_smote$y, logistic_test_score_smote),
                                      auc(bank_test_smote$y, logistic_test_score_smote_2),
                                      auc(bank_test_smote$y, tree_opt_test_score_smote),
                                      auc(bank_test_smote$y, tree_opt_test_score_smote_2),
                                      auc(bank_test_smote$y, rf_test_score_smote)),
                            "Threshold" = c(0.5, 0.5, 0.5, 0.5, 0.5),
                            "Accuracy" = c(logistic_test_confm_smote[["overall"]][["Accuracy"]],
                                           logistic_test_confm_smote_2[["overall"]][["Accuracy"]],
                                           tree_opt_test_confm_smote[["overall"]][["Accuracy"]],
                                           tree_opt_test_confm_smote_2[["overall"]][["Accuracy"]],
                                           rf_test_confm_smote[["overall"]][["Accuracy"]]),
                            "Sensitivity" = c(logistic_test_confm_smote[["byClass"]][["Sensitivity"]],
                                              logistic_test_confm_smote_2[["byClass"]][["Sensitivity"]],
                                              tree_opt_test_confm_smote[["byClass"]][["Sensitivity"]],
                                              tree_opt_test_confm_smote_2[["byClass"]][["Sensitivity"]],
                                              rf_test_confm_smote[["byClass"]][["Sensitivity"]]),
                            stringsAsFactors = F)
df_final_smote
```







## Model selection - conclusion

```{r}
df_final_ns
```

```{r}
df_final_ds
```

```{r}
df_final_us
```

```{r}
df_final_smote
```






```{r, rows.print=25}
df_final_tot = bind_rows(list("ns" = df_final_ns,
                              "ds" = df_final_ds,
                              "us" = df_final_us,
                              "smote" = df_final_smote),
                         .id = "sampling")
df_final_tot
```

```{r, rows.print=25}
df_final_tot %>% 
  arrange(-Accuracy)
```

```{r, rows.print=25}
df_final_tot %>% 
  arrange(-Sensitivity)
```

```{r, rows.print=25}
df_final_tot %>% 
  mutate(Perf = 2*Sensitivity + Accuracy) %>% 
  arrange(-Perf)
```

